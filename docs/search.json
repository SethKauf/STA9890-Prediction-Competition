[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prediction Competition",
    "section": "",
    "text": "Landing Page for Prediction Competition\n\n\n\n\n\n\n\nLast Updated: Sunday 03 23, 2025 at 17:08PM"
  },
  {
    "objectID": "Research_Report_1.html#bias-and-variance",
    "href": "Research_Report_1.html#bias-and-variance",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.1 Bias and Variance",
    "text": "2.1 Bias and Variance\nIn modeling, Bias and Variance each play an important role in building a usable model that can predict some outcome \\(y\\). Both terms combined give us a general error term which in turn can be used to tell us how good a model actually is.\n\n\n\n\nBias and Variance Tradeoff via Wikipedia\n\n\n\nBias is the “unavoidable model error”, that is it looks at the difference between the ground truth and the predicted values of our model. Variance on the other hand, is the squared difference between the predicted values and the mean of the predicted values."
  },
  {
    "objectID": "Research_Report_1.html#mse",
    "href": "Research_Report_1.html#mse",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.2 MSE",
    "text": "2.2 MSE\nA common error term is the Mean Squared Error or MSE. MSE in the 2D-linear world is defined as:\n\\(MSE=\\frac{(y-\\hat y)^2}{n}\\)\nOr y-true less y-predicted squared over the number of samples n. In the world of Machine Learning though, we usually use matrices to fit our models. For example, in OLS regression, we are searching for a matrix-version of a “line of best fit”, which looks like:\n\\(y = X\\beta + \\epsilon\\)\nIn Matrix Notation, the Expected Value breaks out as:\n\\(E[MSE]=Bias^2+Variance \\space (+Irreducible \\space Noise)\\)\nUsing the terminology from the previous section:\n\\(Bias^2=E[(f(X)-E[\\hat f(X)])^2]\\)\n$Variance = E[(f(X)-Ef(X)])^2]\nWhile the Irreducible Noise refers to the fact that, by the nature of how modelling and prediction work, there will simply always be factors that can’t be accounted for. It is denoted by:\n\\(\\sigma^2=E[\\epsilon^2]\\)\nWhere \\(\\epsilon\\) is the error term.\nThe full breakdown of the Expected MSE formula is as follows1:\n\\(MSE(\\hat \\theta)=E_{\\theta}[(\\hat \\theta - \\theta)^2]\\)2\nBreaking this out further:\n\\(=E_{\\theta}[(\\hat \\theta - E_\\theta[\\hat \\theta]+E_\\theta[\\hat \\theta]-\\theta)^2]\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2 + 2(\\hat \\theta - E_\\theta [\\hat \\theta])(E_\\theta [\\hat \\theta] - \\hat \\theta)+(E_\\theta[\\hat \\theta - \\theta)^2]\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+E_\\theta [2(\\hat \\theta - E_\\theta[\\hat \\theta])(E_\\theta[\\hat \\theta]-\\theta)]+E_\\theta[(E_\\theta[\\hat \\theta] - \\theta)^2]\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+2(E_\\theta[\\hat \\theta]-\\theta) E_\\theta[\\hat \\theta - E_\\theta[\\hat \\theta]] + (E_\\theta[\\hat \\theta] - \\theta)^2\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+2(E_\\theta[\\hat \\theta]-\\theta)E_\\theta[\\hat \\theta - E_\\theta[\\hat \\theta]]+(E_\\theta[\\hat \\theta] - \\theta)^2\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+(E_\\theta[\\hat \\theta] - \\theta)^2\\)\nRemembering the Bias and Variance formulas:\n\\(Bias^2=E[(f(X)-E[\\hat f(X)])^2]=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]\\)\n\\(Variance=E[(\\hat f(X)-E[\\hat f(X)])^2]=(E_\\theta[\\hat \\theta] - \\theta)^2\\)\nWith irreducible error term \\(E[\\epsilon]\\)"
  },
  {
    "objectID": "Research_Report_1.html#ols-regression-and-blue",
    "href": "Research_Report_1.html#ols-regression-and-blue",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.3 OLS Regression and BLUE",
    "text": "2.3 OLS Regression and BLUE\nOLS Regression is a simple linear model that tries to fit a line to the data that minimizes the Residual Sum of Squares error term. It is generally considered to fit the criteria of BLUE, that is Best Unbiased Linear Estimator. It is the Best because it has the lowest variance among all other linear estimators, it is Linear because that is the model-type, it is Unbiased because there is no difference between the ground truth and the prediction, specifically the expected value of the predicted parameter is equal to the parameter in the DGP, or \\(E[\\hat \\theta]=\\theta\\), and it is an estimator because it estimates \\(\\theta^2\\). However, there are exceptions to this BLUE property."
  },
  {
    "objectID": "Research_Report_1.html#dgp-and-misspecification",
    "href": "Research_Report_1.html#dgp-and-misspecification",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.4 DGP and Misspecification",
    "text": "2.4 DGP and Misspecification\nA Data Generating Process or DGP is the function of the world that generates data. It is the “ground truth” upon which we try to model the data we do have. Everything discussed until now all is built on-top of this idea that the real-world has unknowns and we can only do our best to try and understand and even predict those unknowns.\nIn the context of OLS Regression, the faults for OLS stem from the fact that it is only a BLUE model-type when the underlying DGP is linear. However, what happens if the real DGP isn’t \\(y=X\\beta+\\epsilon\\), rather it’s \\(y=X^2 +\\epsilon\\) a quadratic equation. This would be model misspecification and would cause issues in trying to estimate parameters our DGP. We can test for whether OLS would be a good model type by looking at various factors. First is an expectation of Homoscedasticity in the data, that is \\(Var(\\epsilon_i|X)=\\sigma^2\\) or that the residuals (that is, \\(e=y-\\hat y\\)) have a constant error variance. Running an OLS model on the Credit Card dataset from ISLR3 with a predicted variable of \\(y\\) being the credit card balance, it produces the following chart:\n\n\n\n\n\n\n\n\n\n\n\n\n Homoscedasticity of \\(\\hat y\\) from ISL Credit Data Set\n\nIf these were more evenly spread out around the mean, this would be a good indication that the OLS model would be the best to use. However, that is not the case here: we see somewhat of a checkmark shape that coalesces in the bottom-left.\nTo check for the unbiased property in the DGP, we can look towards Endogeneity, or that the errors are not affected by the input \\(X\\) variables: \\(E[\\epsilon|X]=0\\). Looking at the input X variables:\n\n\n\n\n\n\n\n\n\n\n\n\n Endogeneity of feature residuals from ISL Credit Data Set\n\nThe spread is not very even here, though it’s not extremely lopsided. These however show that we likely will have bias in our model if we try to use OLS."
  },
  {
    "objectID": "Research_Report_1.html#derivation-for-ols",
    "href": "Research_Report_1.html#derivation-for-ols",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.1 Derivation for OLS",
    "text": "3.1 Derivation for OLS\nOLS seeks to minimize some predictor variable \\(\\hat \\beta\\) to minimize the error function. To do this, we look at the error term that OLS is trying to minimize, then substitute it in to find a proper value for \\(\\hat \\beta\\):\n\\(RSS=\\Sigma^n_{i=1}=||\\epsilon||^2\\)\nSince \\(y=X\\beta+\\epsilon\\) we can rearrange the terms so \\(\\epsilon=y-X\\beta\\), expand the terms, then derive with respect to \\(\\beta\\):\n\\(RSS=||y-X||^2=(y-X\\beta)^T(y-X\\beta)\\)\n\\(\\frac{d}{d\\beta}[(y-X\\beta)^T(y-X\\beta)]=\\frac{d}{d\\beta}[y^Ty-2\\beta^T X^Ty+\\beta^TX^TX\\beta]\\)\nThe \\(y^T y\\) term is a constant while \\(\\frac{d}{d\\beta}[\\beta^T]=1\\) so we get the following and set it equal to 0 to minimize it:\n\\(-2X^Ty+2X^T X\\beta=0\\)\n\\(2X^TX\\beta=2X^T y\\)\n\\(\\beta = \\frac {X^T y} {X^T X}=(X^T X)^{-1} X^T y\\)"
  },
  {
    "objectID": "Research_Report_1.html#ols-gradient",
    "href": "Research_Report_1.html#ols-gradient",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.2 OLS Gradient",
    "text": "3.2 OLS Gradient\nGradient Descent refers to the process of iteratively fitting models to the gradient of a function is the vector field whose value at point p gives the direction and rate of the fastest increase4. In OLS terms, this translates to:\n\\(\\beta^{(k+1)}=\\beta^{k}-c\\nabla\\mathcal{L}|_{\\beta=\\beta^{(k)}}\\) where \\(\\nabla\\mathcal{L}\\) is the loss function on the gradient.\nGradient Descent keeps going until the parameter and objective each converge, or: \\(\\beta^{(k+1)}\\approx\\beta^{(k)}\\) and \\(\\mathcal{L}(\\beta^{(k+1)})\\approx\\mathcal{L}(\\beta^{(k)})\\).\nThe setup is similar to the closed-form derivation from the previous step, it turns into:\n\\(\\beta^{(k+1)}=\\beta^k-c\\mathcal{L}|_{\\beta=\\beta^k}=\\beta^k-c(-2X^Ty+2X^TX\\beta)=\\beta^k-2c(X^TX\\beta^k-X^Ty)\\)\nPlotting it out, it looks like:\n\n\nConverged at iteration 293\n\n\n\n\n\n\n\n Sample Gradient Descent with convergence in OLS"
  },
  {
    "objectID": "Research_Report_2.html",
    "href": "Research_Report_2.html",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "",
    "text": "Research Report 2 focuses on Ensemble Learning Techniques for Fair Classification.\nAs Machine Learning (ML) Systems become more integrated into society, their development and implementation continue to come under scrutiny. One aspect of this scrutiny comes from the idea of fairness, that Machine Learning models should treat people fairly, regardless of characteristics. Usually, this refers to a protected class, such as race, gender, or ethnicity. In broader contexts, it can also refer to other factors such as income."
  },
  {
    "objectID": "Research_Report_1.html#ols-weight-decay",
    "href": "Research_Report_1.html#ols-weight-decay",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.3 OLS Weight Decay",
    "text": "3.3 OLS Weight Decay\nTo try and make up for the issues of overfitting and large parameterization, OLS can implement a process known as weight decay. This adds a penalty term to the gradient formula to penalize large parameters and try to keep the model simple. The set-up is similar to before but with an added penalty term:\n\\(\\beta^{(k+1)}=\\beta^{k}-c\\nabla\\mathcal{L}|_{\\beta=\\beta^{(k)}}-\\mathcal{w}\\beta^{(k)}\\)\nPlugging this into the gradient derivative from the previous section, we get to this step:\n\\(=\\frac{\\partial}{\\partial \\beta}[y^Ty-2\\beta^TX^Ty+\\beta^TX^TX\\beta-\\mathcal{w}\\beta]\\)\nThe \\(y^Ty\\) term disappears since it’s a constant in this context and the \\(\\beta^T\\) terms set to \\(\\frac{\\partial}{\\partial \\beta}[\\beta^T]=1\\)\n\\(\\nabla\\mathcal{L}=-2X^Ty+2X^TX\\beta-\\mathcal{w}\\beta\\)\nPlugging this back into the original formula for \\(\\beta^{(k+1)}\\):\n\\(\\beta^{(k+1)}=\\beta^{(k)}-c(-2X^Ty+2X^TX\\beta-2\\mathcal{w}\\beta)\\)\nWhich finally equals:\n\\(\\beta^{(k+1)}=\\beta^{(k)}-2c(X^TX\\beta^{(k)}-X^Ty-\\mathcal{w}I)\\)\nAnd in simulation-land, we can see this will converge sooner than just regular Gradient Descent from the previous section:\n\n\nCode\n## Simulate Gradient Descent\n\n# Function\ngradient_descent_weight_decay &lt;- function(X, y, alpha = 0.01, lambda_val = 0.1, num_iterations = 1000, tol = 1e-6) {\n  # Add intercept column if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  beta &lt;- rep(0, k)\n  loss_history &lt;- numeric()\n  \n  for (i in 1:num_iterations) {\n    # Compute gradient with weight decay\n    gradient &lt;- 2 * (t(X) %*% X %*% beta - t(X) %*% y + lambda_val * beta) / n\n    beta &lt;- (1 - 2 * alpha * lambda_val) * beta - alpha * gradient\n    \n    # Compute loss (MSE + L2 penalty)\n    loss &lt;- mean((y - X %*% beta)^2) + lambda_val * sum(beta^2)\n    loss_history[i] &lt;- loss\n    \n    # Convergence check\n    if (i &gt; 1 && abs(loss_history[i] - loss_history[i - 1]) &lt; tol) {\n      message(\"Converged at iteration \", i)\n      break\n    }\n  }\n  \n  list(beta = beta, loss_history = loss_history)\n}\n\n\n\n\nConverged at iteration 141\n\n\n\n\n\n\n\n\n\nSample Weight Decay and Gradient Descent with convergence in OLS"
  },
  {
    "objectID": "Research_Report_1.html#ridge-regression",
    "href": "Research_Report_1.html#ridge-regression",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.4 Ridge Regression",
    "text": "3.4 Ridge Regression\nRidge Regression is an \\(\\ell_p-Regression\\) function, specifically \\(\\ell_2\\). It applies a penalty term to the original OLS parameters, \\(\\lambda\\), to minimize the effects of large coefficients and overfitting.\n\\(\\ell_2 (\\beta) = ||y-X\\beta||^2+\\lambda ||\\beta||^2\\)\nBreaking this out then deriving it with respect to β:\n\\(\\ell_2 (\\beta) = (y-X\\beta)^T (y-X\\beta) + \\lambda \\beta^T \\beta = -2X^T y + 2X^T X + 2\\lambda \\beta\\)\nSetting this equal to 0 and solving for \\(\\beta\\):\n\\(-2X^Ty + 2X^T X \\beta + 2 \\lambda \\beta = 0\\)\nWhere \\(\\beta = (X^T X \\lambda I)^{-1} X^T y\\)\nDeriving the gradient term \\(\\beta^{k+1} = \\beta^k - c \\nabla L|_{\\beta = \\beta^k}=\\beta^k-2c(X^T y + X^T X \\beta^k - \\lambda I)\\)\nWhich is the same form as OLS with weight decay: \\(\\beta^k - 2c(X^T y + X^T X \\beta^k - wI)\\)"
  },
  {
    "objectID": "Research_Report_1.html",
    "href": "Research_Report_1.html",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "",
    "text": "Research Report 1 covers the Bias-Variance Trade off in a Linear Data Generating Process (DGP) and the BLUE-ness of Ordinary Least Squares (OLS) Regression. This will include the theoretical background, computation of the gradient descent and weight decay, and bias and variance under both a linear and non-linear DGP. Some basic statistical background is assumed for this report, e.g.: I won’t delve into detail on \\(Y=f(x)+\\epsilon\\), which will be treated as common knowledge."
  },
  {
    "objectID": "Research_Report_1.html#positing-a-linear-dgp-with-monte-carlo",
    "href": "Research_Report_1.html#positing-a-linear-dgp-with-monte-carlo",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "4.1 Positing a Linear DGP with Monte Carlo",
    "text": "4.1 Positing a Linear DGP with Monte Carlo\nIf we assume a Linear DGP of \\(y=X\\beta + \\epsilon\\), we can use Monte Carlo simulations to view bias and variance under these conditions.\n\n\nCode\n# Parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nbeta_true &lt;- c(5, 2)  # Intercept and slope\n\n# Initialize storage\nresults &lt;- data.frame()\n\n# Monte Carlo loop\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n  \n  for (sim in 1:num_simulations) {\n    # Generate training data\n    X &lt;- runif(n, -3, 3)\n    y &lt;- beta_true[1] + beta_true[2] * X + rnorm(n, 0, 1)\n    X_mat &lt;- cbind(1, X)  # Add intercept\n    \n    # Fit model\n    model &lt;- lm(y ~ X)\n    beta_hat &lt;- coef(model)\n    beta_estimates[sim, ] &lt;- beta_hat\n    \n    # In-sample MSE\n    y_pred &lt;- predict(model)\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n    \n    # Generate test set\n    X_test &lt;- runif(n, -3, 3)\n    y_test &lt;- beta_true[1] + beta_true[2] * X_test + rnorm(n, 0, 1)\n    test_df &lt;- data.frame(X = X_test)\n    \n    # Out-of-sample MSE\n    y_test_pred &lt;- predict(model, newdata = test_df)\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n  \n  # Compute bias and variance for β1 (slope)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta_true[2]\n  variance_beta1 &lt;- var(beta1_hat)\n  \n  # Store results\n  results &lt;- rbind(results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Plot Bias, Variance, and MSEs\npar(mfrow = c(2, 2))\n\n# Plot Bias\np1 &lt;- ggplot(results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size\", x = \"Sample Size (n)\", y = \"Bias\")\n\n# Plot Variance\np2 &lt;- ggplot(results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size\", x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-Sample MSE\np3 &lt;- ggplot(results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size\", x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-Sample MSE\np4 &lt;- ggplot(results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size\", x = \"Sample Size (n)\", y = \"MSE\")\n\n# Arrange in 2x2 grid\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nBias and Variance under a Linear DGP with Monte Carlo simulations"
  },
  {
    "objectID": "Research_Report_1.html#repeating-with-ridge-regression",
    "href": "Research_Report_1.html#repeating-with-ridge-regression",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "4.2 Repeating with Ridge Regression",
    "text": "4.2 Repeating with Ridge Regression\nLet’s repeat the Monte Carlo simulations using Ridge Regression to see the difference between it and OLS\n\n\nCode\n# Set simulation parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nbeta_true &lt;- c(5, 2)\nlambda_ridge &lt;- 1.0  # L2 regularization strength\n\n# Storage for results\nridge_results &lt;- data.frame()\n\n# Loop over sample sizes\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n\n  for (sim in 1:num_simulations) {\n    # Generate training data\n    X &lt;- runif(n, -3, 3)\n    y &lt;- beta_true[1] + beta_true[2] * X + rnorm(n)\n    X_mat &lt;- cbind(1, X)  # Add intercept column\n\n    # Fit Ridge regression (glmnet expects matrix input, no intercept term)\n    ridge_model &lt;- glmnet(x = X_mat, y = y, alpha = 0, lambda = lambda_ridge, intercept = FALSE, standardize = FALSE)\n    beta_hat &lt;- as.vector(coef(ridge_model, s = lambda_ridge))[-1]  # drop intercept row\n    beta_estimates[sim, ] &lt;- beta_hat\n\n    # In-sample prediction\n    y_pred &lt;- X_mat %*% beta_hat\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n\n    # Test set\n    X_test &lt;- runif(n, -3, 3)\n    y_test &lt;- beta_true[1] + beta_true[2] * X_test + rnorm(n)\n    X_test_mat &lt;- cbind(1, X_test)\n\n    # Out-of-sample prediction\n    y_test_pred &lt;- X_test_mat %*% beta_hat\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n\n  # Bias and variance of β1 (slope)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta_true[2]\n  variance_beta1 &lt;- var(beta1_hat)\n\n  # Save results\n  ridge_results &lt;- rbind(ridge_results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Bias\np1 &lt;- ggplot(ridge_results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"Bias\")\n\n# Variance\np2 &lt;- ggplot(ridge_results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-sample MSE\np3 &lt;- ggplot(ridge_results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-sample MSE\np4 &lt;- ggplot(ridge_results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Display all four plots in 2x2 layout\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n Bias and Variance under a Linear DGP with Monte Carlo simulations using Ridge Regression\n\nWhile we can see the bias is always a little worse in Ridge Regression, the variance takes a sharper decline; eventually we end up with a better out-of-sample MSE than we did with OLS."
  },
  {
    "objectID": "Research_Report_1.html#monte-carlo-simulation",
    "href": "Research_Report_1.html#monte-carlo-simulation",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "5.1 Monte Carlo Simulation",
    "text": "5.1 Monte Carlo Simulation\nSimilar to the previous section, here we will look at how bias and variance are affected when the DGP is non-linear, \\(y=X^2 \\beta + \\epsilon\\)\n\n\nCode\n# Parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nlambda_ridge &lt;- 1.0\nbeta &lt;- c(5, 2)  # Intercept and quadratic coefficient\n\n# Function to simulate non-linear DGP: y = β0 + β1 * x^2 + ε\ngenerate_nonlinear_dgp &lt;- function(n, beta = c(5, 2), noise_sd = 1.0) {\n  x &lt;- runif(n, -3, 3)\n  y &lt;- beta[1] + beta[2] * x^2 + rnorm(n, sd = noise_sd)\n  return(list(x = x, y = y))\n}\n\n# Storage for results\nridge_nonlinear_results &lt;- data.frame()\n\n# Simulation\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n\n  for (sim in 1:num_simulations) {\n    # Generate training data\n    train &lt;- generate_nonlinear_dgp(n)\n    x &lt;- train$x\n    y &lt;- train$y\n    X_quad &lt;- cbind(1, x^2)\n\n    # Fit Ridge regression\n    ridge_model &lt;- glmnet(x = X_quad, y = y, alpha = 0, lambda = lambda_ridge,\n                          intercept = FALSE, standardize = FALSE)\n    beta_hat &lt;- as.vector(coef(ridge_model, s = lambda_ridge))[-1]\n    beta_estimates[sim, ] &lt;- beta_hat\n\n    # In-sample MSE\n    y_pred &lt;- X_quad %*% beta_hat\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n\n    # Test set\n    test &lt;- generate_nonlinear_dgp(n)\n    x_test &lt;- test$x\n    y_test &lt;- test$y\n    X_test_quad &lt;- cbind(1, x_test^2)\n    y_test_pred &lt;- X_test_quad %*% beta_hat\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n\n  # Bias and variance for β1 (quadratic term)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta[2]\n  variance_beta1 &lt;- var(beta1_hat)\n\n  # Store results\n  ridge_nonlinear_results &lt;- rbind(ridge_nonlinear_results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Bias\np1 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Bias\")\n\n# Variance\np2 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-sample MSE\np3 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-sample MSE\np4 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Display 2x2 grid\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\nThe bias is now nearly constant as seen in the top-left, while the in-and-out of sample errors seem to still follow the trends we saw with the last two."
  },
  {
    "objectID": "Research_Report_1.html#monte-carlo-simulation-for-non-linear-dgp",
    "href": "Research_Report_1.html#monte-carlo-simulation-for-non-linear-dgp",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "5.1 Monte Carlo Simulation for non-Linear DGP",
    "text": "5.1 Monte Carlo Simulation for non-Linear DGP\nSimilar to the previous section, here we will look at how bias and variance are affected when the DGP is non-linear, \\(y=X^2 \\beta + \\epsilon\\)\n\n\nCode\n# Parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nlambda_ridge &lt;- 1.0\nbeta &lt;- c(5, 2)  # Intercept and quadratic coefficient\n\n# Function to simulate non-linear DGP: y = β0 + β1 * x^2 + ε\ngenerate_nonlinear_dgp &lt;- function(n, beta = c(5, 2), noise_sd = 1.0) {\n  x &lt;- runif(n, -3, 3)\n  y &lt;- beta[1] + beta[2] * x^2 + rnorm(n, sd = noise_sd)\n  return(list(x = x, y = y))\n}\n\n# Storage for results\nridge_nonlinear_results &lt;- data.frame()\n\n# Simulation\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n\n  for (sim in 1:num_simulations) {\n    # Generate training data\n    train &lt;- generate_nonlinear_dgp(n)\n    x &lt;- train$x\n    y &lt;- train$y\n    X_quad &lt;- cbind(1, x^2)\n\n    # Fit Ridge regression\n    ridge_model &lt;- glmnet(x = X_quad, y = y, alpha = 0, lambda = lambda_ridge,\n                          intercept = FALSE, standardize = FALSE)\n    beta_hat &lt;- as.vector(coef(ridge_model, s = lambda_ridge))[-1]\n    beta_estimates[sim, ] &lt;- beta_hat\n\n    # In-sample MSE\n    y_pred &lt;- X_quad %*% beta_hat\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n\n    # Test set\n    test &lt;- generate_nonlinear_dgp(n)\n    x_test &lt;- test$x\n    y_test &lt;- test$y\n    X_test_quad &lt;- cbind(1, x_test^2)\n    y_test_pred &lt;- X_test_quad %*% beta_hat\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n\n  # Bias and variance for β1 (quadratic term)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta[2]\n  variance_beta1 &lt;- var(beta1_hat)\n\n  # Store results\n  ridge_nonlinear_results &lt;- rbind(ridge_nonlinear_results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Bias\np1 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Bias\")\n\n# Variance\np2 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-sample MSE\np3 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-sample MSE\np4 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Display 2x2 grid\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n Bias and Variance of non-Linear DGP using Monte Carlo simulations\n\nThe bias is now nearly constant as seen in the top-left, while the in-and-out of sample errors seem to still follow the trends we saw with the last two."
  },
  {
    "objectID": "Research_Report_1.html#footnotes",
    "href": "Research_Report_1.html#footnotes",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMSE Wikipedia↩︎\nGauss-Markov Theorem Wikipedia↩︎\nISL Credit Card Data↩︎\nGradient Wikipedia↩︎"
  },
  {
    "objectID": "Research_Report_2.html#examples-of-unfairness",
    "href": "Research_Report_2.html#examples-of-unfairness",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "2.1 Examples of Unfairness",
    "text": "2.1 Examples of Unfairness\nBelow are two examples of societal unfairness baked into ML Systems.\n\n2.1.1 COMPAS\nThe COMPAS algorithm developed by the firm Equivant is commonly used by judges and forecasts which criminals are most likely to reoffend, using a quantitative approach to fairness. Although it was correctly predicting recidivism rates for defendants regardless of rates (a good True Positive rate), it was scoring black people who would not re-offend at twice the risk of recidivism as their white counterparts (bad False Positive rate). While Equivant argued that because their model was correctly predicting recidivism rates when a defendant did reoffend, it proved their model was unbiased. ProPublica, who led the research into their model’s False Positives, argued against the misclassification based on race that occurred in the model.\n\n\n2.1.2 Amazon\nAmazon has been implementing ML Systems in many of its processes for years now. One of these processes, their hiring process, came under scrutiny in 2018 after their ML team discovered that the model used to “rate” candidates in the hiring process was discriminating against women. This was due to the tech industry in-general being largely made up of men, which was reflected in the resumes submitted to the company over a 10-year period. The model effectively self-taught itself to be more male-centric, penalizing resumes that had terms such as “women” in the scoring metric (ie if the resume said “captain of women’s chess team”). This is also a reflection of how the STEM field is in-general, which is a mostly male-dominated industry, hence why most of the resumes coming in were from men. This is an example of societal bias being used in an ML context."
  },
  {
    "objectID": "Research_Report_2.html#measuring-fairness",
    "href": "Research_Report_2.html#measuring-fairness",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "2.2 Measuring Fairness",
    "text": "2.2 Measuring Fairness\nA common approach to measuring fairness is the aforementioned deviation from demographic parity. If we divide the populations into two groups, \\(G_1\\) and \\(G_2\\), the deviation from demographic parity associated with a classifier \\(f : R^p \\rightarrow {0,1}\\) is given by:\n\\[\n\\text{DDP}(f) = \\left| \\frac{1}{|G_1|} \\sum_{i \\in G_1} f(x_i) - \\frac{1}{|G_2|} \\sum_{i \\in G_2} f(x_i) \\right|\n= \\left| \\mathbb{E}_{x \\sim G_1}[f(x)] - \\mathbb{E}_{x \\sim G_2}[f(x)] \\right|\n\\]"
  },
  {
    "objectID": "Research_Report_2.html#target-class",
    "href": "Research_Report_2.html#target-class",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "3.1 Target Class",
    "text": "3.1 Target Class\nThe target class in this dataset is the READMITTED column which has three values: No, &lt;30 and &gt;30. This feature asks whether a patient was readmitted following their initial hospital visit within 30 days (early readmission) or after 30 days (general readmission). Its raw data contains 101,766 records with 50 columns (\\(101,766 \\space \\times \\space 50\\)) and a secondary spreadsheet used to connect three id columns in the data to an English meaning (ie admission_type_id = 1 is an Emergency).\nThis target was turned into two potential target-y classes: readmit_early and readmit_at_all. The former is just the values of &lt;30 while the latter is either &lt;30 or &gt;30. Without any data cleaning or feature engineering, the class imbalance for readmit_early is about 88.8% : 11.2% for 0 : 1, while readmit_at_all is about 53.9% : 46.1% for 0 : 1. Although there is value in looking into readmit_early as a target, the class imbalance widens following the data cleaning section and it becomes a virtually untenable target class without any sort of artificial balancing such as the SMOTE method.\nAs a note: the data cleaning and EDA was done in Python to create the eventual modeling data. The full cleaning and EDA code can be found on my GitHub if interested.\n\n\n\n\nTarget class"
  },
  {
    "objectID": "Research_Report_2.html#data-cleaning",
    "href": "Research_Report_2.html#data-cleaning",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "3.2 Data Cleaning",
    "text": "3.2 Data Cleaning\nThe first step to dealing with the data was cleaning it. This started by checking for NULLs, which in this dataset was denoted by the presence of ? in the data. Only 7 of the 50 original features have NULLs. race, diag_1, diag_2, and diag_3 (diagnoses 1, 2 and 3 respectively) have &lt; 3% of their records as NULL. payer_code and medical_specialty (insurance provider and medical specialty of the overseeing physician) are about 40% NULL.\n\n\n\n\nNULL counts represented by “?”\n\n\n\nFor these 6 columns, I elected to remove all records with the NULL values in any of these columns. This removed about 40% of the data, bringing the total number of records down to 48,140.\nThe 7th column, weight, was comprised of nearly 97% NULL data. Once I accounted for the NULLs in the first 6 columns, only about 3,000 of the 48,140 records actually had a filled value for weight. This is actually extremely unfortunate as weight is both a demographic feature and an especially important feature in something like diabetes. However, there is simply too much data lost by keeping it at the expense of ~45,000 records, while imputation of some sort (ie mean, forward fill, random-fill, etc. imputation) would be too sketchy given the sheer volume of NULL values. Because of this, the weight feature has been completely dropped from the model.\nAnother consideration in the dataset is that some patients are “repeat” patients. Looking at the distribution of these patients, we see most only visit the hospital once, with a median of 1, an upper bound on the IQR of 2, and a mean of ~1.49 visits. With this in mind, any patient whose number appears in the data more than 3 times was removed. Further, because this still means there are some duplicates, when it comes time to split the data, it will be split on patient_nbr to avoid patients appearing in both the training and testing data.\n\n\n\n\nRepeat patient info"
  },
  {
    "objectID": "Research_Report_2.html#feature-engineering",
    "href": "Research_Report_2.html#feature-engineering",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "3.3 Feature Engineering",
    "text": "3.3 Feature Engineering\nThe next step was to engineer some features using standard techniques:\n\nCategorical encoding\n\nReally just left the id columns as-is for this one\nDebatable if this is a good idea or not since there’s no inherent order to the numbers\n\nOrdinal encoding\n\nEncoding the age-brackets to a 0-9 scale\n\nDummy encoding\n\nrace \\(\\rightarrow\\) race_AfricanAmerican, race_Caucasian…\n\nBinarizing\n\nGlucose Levels \\(\\rightarrow\\) High Glucose Indicator\n\nScaling numeric data\n\nIe num_visists\n\n“Other” grouping\n\nThreshold for diagnoses columns at &lt; 100 instances\nThreshold for race, medical_specialty, payer_code at &lt; 1000 instances"
  },
  {
    "objectID": "Research_Report_2.html#feature-selection",
    "href": "Research_Report_2.html#feature-selection",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "3.4 Feature Selection",
    "text": "3.4 Feature Selection\nWith the NULL values dealt with and the features engineered, the final step before modeling was feature selection. Due to the dummy encoding, there were a few hundred features to choose from. Most of these new features were based on the diagnoses 1-3 columns, since they each had a lot of values. With this in mind, checking the correlation between those dummy columns and the y-target readmit_at_all, only the top-50 diagnoses features were kept based on absolute correlation. These 50 diagnoses features and 37 other features, including dummy encoded race variables, age brackets and gender, were kept in the modeling data. The final dataset contains both potential y targets of readmit_at_all and readmit_early, however, the latter (along with patient_nbr) is dropped once it comes time to modeling."
  },
  {
    "objectID": "Research_Report_2.html#naïve-bayes",
    "href": "Research_Report_2.html#naïve-bayes",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "4.2 Naïve Bayes",
    "text": "4.2 Naïve Bayes\n\n\nCode\n# Model\nnb_model &lt;- naiveBayes(readmit_at_all ~ ., data = train_data)\n\n# Predict\nnb_pred &lt;- predict(nb_model, test_data)\n\n# Predict proba\nnb_prob &lt;- predict(nb_model, test_data, type=\"raw\")[, 2]\n\n# Create a tibble for yardstick\nresults &lt;- tibble(\n  truth = factor(test_data$readmit_at_all),\n  prediction = factor(nb_pred)\n)\n\n# Build the confusion matrix\nnb_cm &lt;- confusionMatrix(as.factor(nb_pred), as.factor(test_data$readmit_at_all))\n\n# Convert to data frame\nnb_cm_df &lt;- as.data.frame(nb_cm$table)\n\n# Plot with ggplot2\nggplot(data = nb_cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 5) +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  labs(\n    title = \"Naive Bayes Confusion Matrix\",\n    x = \"Actual Label\",\n    y = \"Predicted Label\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Accuracy\naccuracy &lt;- mean(nb_pred == test_data$readmit_at_all)\ncat(\"Accuracy:\", round(accuracy, 4), \"\\n\")\n\n\nAccuracy: 0.5679"
  },
  {
    "objectID": "Research_Report_2.html#linear-discriminant-analysis-lda",
    "href": "Research_Report_2.html#linear-discriminant-analysis-lda",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "4.3 Linear Discriminant Analysis (LDA)",
    "text": "4.3 Linear Discriminant Analysis (LDA)\n\n\nCode\n# Model\nlda_model &lt;- lda(readmit_at_all ~ ., data = train_data)\n\n# Predict on test set\nlda_pred &lt;- predict(lda_model, test_data)\n\n# Confusion matrix (and extract accuracy)\ncm &lt;- confusionMatrix(as.factor(lda_pred$class), as.factor(test_data$readmit_at_all))\n\ncm_df &lt;- as.data.frame(cm$table)\nggplot(data = cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 5) +\n  scale_fill_gradient(low = \"lightblue\", high = \"steelblue\") +\n  labs(title = \"LDA Confusion Matrix\", x = \"Actual\", y = \"Predicted\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nlda_accuracy &lt;- cm$overall[\"Accuracy\"]\n\ncat(\"LDA Accuracy:\", round(lda_accuracy, 4), \"\\n\")\n\n\nLDA Accuracy: 0.6364"
  },
  {
    "objectID": "Research_Report_2.html#support-vector-machine-svm-with-cvx",
    "href": "Research_Report_2.html#support-vector-machine-svm-with-cvx",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "4.4 Support Vector Machine (SVM) with CVX",
    "text": "4.4 Support Vector Machine (SVM) with CVX\n\n\nCode\n# X and y prep using pipe\nX_train &lt;- train_data |&gt;\n  dplyr::select(-readmit_at_all) |&gt;\n  as.matrix()\n\ny_train &lt;- ifelse(train_data$readmit_at_all == 1, 1, -1)\n\nX_test &lt;- test_data |&gt;\n  dplyr::select(-readmit_at_all) |&gt;\n  as.matrix()\n\ny_test &lt;- test_data$readmit_at_all\n\nn &lt;- nrow(X_train)\np &lt;- ncol(X_train)\n\nw &lt;- Variable(p)\nb &lt;- Variable(1)\nC &lt;- 1  # You can tune this\n\n# Hinge loss\nhinge_loss &lt;- sum(pos(1 - multiply(y_train, X_train %*% w + b)))\n\n# Objective\nobjective &lt;- Minimize(0.5 * sum_squares(w) + C * hinge_loss)\nproblem &lt;- Problem(objective)\n\nresult &lt;- solve(problem)\n\n# Predicted margin values\nmargin_scores &lt;- X_test %*% result$getValue(w) + result$getValue(b)\n\n# Convert to class predictions\nsvm_pred &lt;- ifelse(margin_scores &gt;= 0, 1, 0)\n\n# Generate confusion matrix object\nsvm_cm &lt;- confusionMatrix(as.factor(svm_pred), as.factor(y_test))\n\n# Convert table to data frame\nsvm_cm_df &lt;- as.data.frame(svm_cm$table)\n\n# Plot the heatmap\nggplot(data = svm_cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 5) +\n  scale_fill_gradient(low = \"white\", high = \"darkgreen\") +\n  labs(\n    title = \"SVM with CVXR Confusion Matrix\",\n    x = \"Actual Label\",\n    y = \"Predicted Label\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nsvm_cm &lt;- confusionMatrix(as.factor(svm_pred), as.factor(y_test))\nsvm_accuracy &lt;- svm_cm$overall[\"Accuracy\"]\n\ncat(\"SVM (CVXR) Accuracy:\", round(svm_accuracy, 4), \"\\n\")\n\n\nSVM (CVXR) Accuracy: 0.6307"
  },
  {
    "objectID": "Research_Report_2.html#logistic-regression-with-cvx",
    "href": "Research_Report_2.html#logistic-regression-with-cvx",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "4.5 Logistic Regression with CVX",
    "text": "4.5 Logistic Regression with CVX\n\n\nCode\n# LogReg with CVX #\n\n# --- Shared Setup ---\nn &lt;- nrow(X_train)\np &lt;- ncol(X_train)\n\n# Define CVXR variables\nbeta &lt;- Variable(p)\nintercept &lt;- Variable(1)\nmargin &lt;- X_train %*% beta + intercept\n\n# Prediction function\npredict_logit &lt;- function(result, beta, intercept, X) {\n  b &lt;- result$getValue(beta)\n  i &lt;- result$getValue(intercept)\n  1 / (1 + exp(- (X %*% b + i)))\n}\n\n# 1. Plain\n\nlog_loss_plain &lt;- sum(logistic(- y_train * margin))\nproblem_plain &lt;- Problem(Minimize(log_loss_plain))\nresult_plain &lt;- solve(problem_plain, solver = \"ECOS\")\n\nprob_pred_plain &lt;- predict_logit(result_plain, beta, intercept, X_test)\npred_class_plain &lt;- ifelse(prob_pred_plain &gt;= 0.5, 1, 0)\ncm_plain &lt;- confusionMatrix(as.factor(pred_class_plain), as.factor(y_test))\ndf_plain &lt;- as.data.frame(cm_plain$table)\ndf_plain$model &lt;- \"Plain\"\n\n\n# 2. Ridge\n\nlambda_ridge &lt;- 1\nlog_loss_ridge &lt;- sum(logistic(- y_train * margin)) + lambda_ridge * sum_squares(beta)\nproblem_ridge &lt;- Problem(Minimize(log_loss_ridge))\nresult_ridge &lt;- solve(problem_ridge, solver = \"ECOS\")\n\nprob_pred_ridge &lt;- predict_logit(result_ridge, beta, intercept, X_test)\npred_class_ridge &lt;- ifelse(prob_pred_ridge &gt;= 0.5, 1, 0)\ncm_ridge &lt;- confusionMatrix(as.factor(pred_class_ridge), as.factor(y_test))\ndf_ridge &lt;- as.data.frame(cm_ridge$table)\ndf_ridge$model &lt;- \"Ridge\"\n\n# 3. Lasso\n\nlambda_lasso &lt;- 1\nlog_loss_lasso &lt;- sum(logistic(- y_train * margin)) + lambda_lasso * norm1(beta)\nproblem_lasso &lt;- Problem(Minimize(log_loss_lasso))\nresult_lasso &lt;- solve(problem_lasso, solver = \"ECOS\")\n\nprob_pred_lasso &lt;- predict_logit(result_lasso, beta, intercept, X_test)\npred_class_lasso &lt;- ifelse(prob_pred_lasso &gt;= 0.5, 1, 0)\ncm_lasso &lt;- confusionMatrix(as.factor(pred_class_lasso), as.factor(y_test))\ndf_lasso &lt;- as.data.frame(cm_lasso$table)\ndf_lasso$model &lt;- \"Lasso\"\n\n# Combine for plotting\ndf_all &lt;- bind_rows(df_plain, df_ridge, df_lasso)\n\n# Create individual plots\nplot_cm &lt;- function(df, title, fill_color) {\n  ggplot(df, aes(x = Reference, y = Prediction, fill = Freq)) +\n    geom_tile(color = \"white\") +\n    geom_text(aes(label = Freq), size = 5) +\n    scale_fill_gradient(low = \"white\", high = fill_color) +\n    labs(title = title, x = \"Actual\", y = \"Predicted\") +\n    theme_minimal()\n}\n\np1 &lt;- plot_cm(df_plain, \"Plain LogReg\", \"firebrick\")\np2 &lt;- plot_cm(df_ridge, \"Ridge LogReg\", \"steelblue\")\np3 &lt;- plot_cm(df_lasso, \"Lasso LogReg\", \"darkgreen\")\n\n# Combine plots\n(p1 | p2 | p3) + plot_layout(guides = \"collect\") & theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nCode\n# Accuracy\ncat(\"Plain Accuracy:\", round(cm_plain$overall[\"Accuracy\"], 4), \"\\n\")\n\n\nPlain Accuracy: 0.6376 \n\n\nCode\ncat(\"Ridge Accuracy:\", round(cm_ridge$overall[\"Accuracy\"], 4), \"\\n\")\n\n\nRidge Accuracy: 0.6385 \n\n\nCode\ncat(\"Lasso Accuracy:\", round(cm_lasso$overall[\"Accuracy\"], 4), \"\\n\")\n\n\nLasso Accuracy: 0.6379"
  },
  {
    "objectID": "Research_Report_2.html#decision-trees",
    "href": "Research_Report_2.html#decision-trees",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "4.6 Decision Trees",
    "text": "4.6 Decision Trees\n\n\nCode\n# Decision Tree Classifier #\n\n# Make sure outcome is a factor for classification\ntrain_data$readmit_at_all &lt;- as.factor(train_data$readmit_at_all)\ntest_data$readmit_at_all  &lt;- as.factor(test_data$readmit_at_all)\n\n# Train single decision tree\ntree_model &lt;- rpart(\n  readmit_at_all ~ ., \n  data = train_data,\n  method = \"class\",\n  control = rpart.control(\n    minsplit = 10,        # allow smaller groups to split\n    cp = 0.001,            # complexity parameter, lower = more splits\n    maxdepth = 10          # allow deeper trees\n  )\n)\n\n# Predict on test data\ntree_preds &lt;- predict(tree_model, newdata = test_data, type = \"class\")\n\n# Evaluate\n# Confusion matrix\ntree_cm &lt;- confusionMatrix(tree_preds, test_data$readmit_at_all)\n\n# Accuracy\ntree_accuracy &lt;- tree_cm$overall[\"Accuracy\"]\ncat(\"Decision Tree Accuracy:\", round(tree_accuracy, 4), \"\\n\")\n\n\nDecision Tree Accuracy: 0.6318 \n\n\nCode\n# Convert to data frame for plotting\ntree_cm_df &lt;- as.data.frame(tree_cm$table)\n\n# Plot\nggplot(data = tree_cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 5) +\n  scale_fill_gradient(low = \"white\", high = \"darkorange\") +\n  labs(\n    title = \"Decision Tree Confusion Matrix\",\n    x = \"Actual Label\",\n    y = \"Predicted Label\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# visualize\nrpart.plot(tree_model, type = 3, extra = 101, fallen.leaves = TRUE)"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html",
    "href": "Workfiles/Report_2_Workfile.html",
    "title": "Import",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport src.workfile_functions as wf\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\npd.set_option('display.max_columns', None)  # Show all columns\npd.set_option('display.expand_frame_repr', False)  # Don't wrap to new lines\ndf = pd.read_csv('../data/RR2/diabetic_data.csv')\nids = pd.read_csv('../data/RR2/IDS_mapping.csv',header=None)\ndf.head()\n\n\n\n\n\n\n\n\nencounter_id\npatient_nbr\nrace\ngender\nage\nweight\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nmax_glu_serum\nA1Cresult\nmetformin\nrepaglinide\nnateglinide\nchlorpropamide\nglimepiride\nacetohexamide\nglipizide\nglyburide\ntolbutamide\npioglitazone\nrosiglitazone\nacarbose\nmiglitol\ntroglitazone\ntolazamide\nexamide\ncitoglipton\ninsulin\nglyburide-metformin\nglipizide-metformin\nglimepiride-pioglitazone\nmetformin-rosiglitazone\nmetformin-pioglitazone\nchange\ndiabetesMed\nreadmitted\n\n\n\n\n0\n2278392\n8222157\nCaucasian\nFemale\n[0-10)\n?\n6\n25\n1\n1\n?\nPediatrics-Endocrinology\n41\n0\n1\n0\n0\n0\n250.83\n?\n?\n1\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNO\n\n\n1\n149190\n55629189\nCaucasian\nFemale\n[10-20)\n?\n1\n1\n7\n3\n?\n?\n59\n0\n18\n0\n0\n0\n276\n250.01\n255\n9\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\nCh\nYes\n&gt;30\n\n\n2\n64410\n86047875\nAfricanAmerican\nFemale\n[20-30)\n?\n1\n1\n7\n2\n?\n?\n11\n5\n13\n2\n0\n1\n648\n250\nV27\n6\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nYes\nNO\n\n\n3\n500364\n82442376\nCaucasian\nMale\n[30-40)\n?\n1\n1\n7\n2\n?\n?\n44\n1\n16\n0\n0\n0\n8\n250.43\n403\n7\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\nCh\nYes\nNO\n\n\n4\n16680\n42519267\nCaucasian\nMale\n[40-50)\n?\n1\n1\n7\n1\n?\n?\n51\n0\n8\n0\n0\n0\n197\n157\n250\n5\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nCh\nYes\nNO\n# this is a weird table with 3 tables in it\nids.head()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nadmission_type_id\ndescription\n\n\n1\n1\nEmergency\n\n\n2\n2\nUrgent\n\n\n3\n3\nElective\n\n\n4\n4\nNewborn\nlen(ids)\n\n68"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#readmission-target-y",
    "href": "Workfiles/Report_2_Workfile.html#readmission-target-y",
    "title": "Import",
    "section": "Readmission (target \\(y\\))",
    "text": "Readmission (target \\(y\\))\n\ndf['readmitted'].value_counts()\n\nreadmitted\nNO     54864\n&gt;30    35545\n&lt;30    11357\nName: count, dtype: int64\n\n\n\nCheckpoint 1\n\n# checkpoint 1\n# df_copy_1 = wf.checkpoint(df, existing_copy=globals().get('df_copy_1'))\n\n\n# df = df_copy_1.copy(deep=True)\n\n\n# create a binary 0-1 for early readmittance\ndf['readmit_early'] = df['readmitted'].apply(lambda x: 1 if x == '&lt;30' else 0)\n\n# create a binary 0-1 for any readmittance\ndf['readmit_at_all'] = df['readmitted'].apply(lambda x: 0 if x == 'NO' else 1)\n\n\ndf[['readmit_early','readmit_at_all']].value_counts()\n\nreadmit_early  readmit_at_all\n0              0                 54864\n               1                 35545\n1              1                 11357\nName: count, dtype: int64\n\n\n\ndf.drop('readmitted',axis=1,inplace=True)\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nencounter_id\npatient_nbr\nrace\ngender\nage\nweight\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nmax_glu_serum\nA1Cresult\nmetformin\nrepaglinide\nnateglinide\nchlorpropamide\nglimepiride\nacetohexamide\nglipizide\nglyburide\ntolbutamide\npioglitazone\nrosiglitazone\nacarbose\nmiglitol\ntroglitazone\ntolazamide\nexamide\ncitoglipton\ninsulin\nglyburide-metformin\nglipizide-metformin\nglimepiride-pioglitazone\nmetformin-rosiglitazone\nmetformin-pioglitazone\nchange\ndiabetesMed\nreadmit_early\nreadmit_at_all\n\n\n\n\n0\n2278392\n8222157\nCaucasian\nFemale\n[0-10)\n?\n6\n25\n1\n1\n?\nPediatrics-Endocrinology\n41\n0\n1\n0\n0\n0\n250.83\n?\n?\n1\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n0\n0\n\n\n1\n149190\n55629189\nCaucasian\nFemale\n[10-20)\n?\n1\n1\n7\n3\n?\n?\n59\n0\n18\n0\n0\n0\n276\n250.01\n255\n9\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\nCh\nYes\n0\n1\n\n\n2\n64410\n86047875\nAfricanAmerican\nFemale\n[20-30)\n?\n1\n1\n7\n2\n?\n?\n11\n5\n13\n2\n0\n1\n648\n250\nV27\n6\nNaN\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nYes\n0\n0"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#nulls",
    "href": "Workfiles/Report_2_Workfile.html#nulls",
    "title": "Import",
    "section": "NULLS",
    "text": "NULLS\n\n# count instances of '?' which is this data's version of NULL\nwf.count_values(df,'?')\n\nColumn                                 ? Count    % of Rows\n------------------------------------------------------------\nweight                                  98,569       96.86%\nmedical_specialty                       49,949       49.08%\npayer_code                              40,256       39.56%\nrace                                     2,273        2.23%\ndiag_3                                   1,423        1.40%\ndiag_2                                     358        0.35%\ndiag_1                                      21        0.02%\n\n\n\ndf['race'].value_counts()\n\nrace\nCaucasian          76099\nAfricanAmerican    19210\n?                   2273\nHispanic            2037\nOther               1506\nAsian                641\nName: count, dtype: int64\n\n\n\ndf = df[(df['race']!='?')]\n\n\ndf['race'].value_counts()\n\nrace\nCaucasian          76099\nAfricanAmerican    19210\nHispanic            2037\nOther               1506\nAsian                641\nName: count, dtype: int64\n\n\n\ndf['gender'].value_counts()\n\ngender\nFemale             53575\nMale               45917\nUnknown/Invalid        1\nName: count, dtype: int64\n\n\n\ndf = df[(df['gender']!='Unknown/Invalid')]\ndf['gender'].value_counts()\n\ngender\nFemale    53575\nMale      45917\nName: count, dtype: int64\n\n\n\nCheckpoint 2\n\n# checkpoint 2\n# df_copy_2 = wf.checkpoint(df, existing_copy=globals().get('df_copy_2'))\n\n\n#df = df_copy_2.copy(deep=True)\n\n\nwf.count_values(df,'?')\n\nColumn                                 ? Count    % of Rows\n------------------------------------------------------------\nweight                                  96,433       96.93%\nmedical_specialty                       48,766       49.01%\npayer_code                              39,711       39.91%\ndiag_3                                   1,349        1.36%\ndiag_2                                     336        0.34%\ndiag_1                                      19        0.02%\n\n\n\n# small ones first\ndf = df[(df['diag_1']!='?')&(df['diag_2']!='?')&(df['diag_3']!='?')]\nlen(df)\n\n98052\n\n\n\n# let's remove any NULLs from payer_code\ndf = df[(df['payer_code']!='?')]\nlen(df)\n\n59129\n\n\n\n# Example usage:\nwf.count_values(df,'?')\n\nColumn                                 ? Count    % of Rows\n------------------------------------------------------------\nweight                                  56,909       96.25%\nmedical_specialty                       32,374       54.75%\n\n\n\nlen(df[df['medical_specialty']!='?'])\n\n26755\n\n\n\nlen(df[df['weight']!='?'])\n\n2220\n\n\nI think it’s reasonable to drop the weight column entirely while imputing NULL-ness on the medical_specialty column.\n\ndf.drop('weight',axis=1,inplace=True)\n\n\ndf['medical_specialty'] = df['medical_specialty'].replace('?', 'Unknown')\n\n\ndf['medical_specialty'].value_counts().head(5)\n\nmedical_specialty\nUnknown                   32374\nEmergency/Trauma           6987\nInternalMedicine           5904\nFamily/GeneralPractice     3146\nCardiology                 2313\nName: count, dtype: int64\n\n\n\ndf['medical_specialty'] = df['medical_specialty'].replace('PhysicianNotFound', 'Unknown')\n\n\nfor col in ['race', 'medical_specialty', 'payer_code']:\n    df = wf.group_rare_categories(df, column=col, threshold=1000)\n\n\ndf['race'].value_counts()\n\nrace\nCaucasian          47072\nAfricanAmerican     9842\nOther               1196\nHispanic            1019\nName: count, dtype: int64\n\n\n\ndf['medical_specialty'].value_counts()\n\nmedical_specialty\nUnknown                   32378\nEmergency/Trauma           6987\nInternalMedicine           5904\nOther                      5527\nFamily/GeneralPractice     3146\nCardiology                 2313\nSurgery-General            1835\nRadiologist                1039\nName: count, dtype: int64\n\n\n\ndf['payer_code'].value_counts()\n\npayer_code\nMC       31404\nHM        6058\nSP        4653\nBC        4462\nMD        3382\nCP        2431\nUN        2219\nCM        1914\nOther     1593\nOG        1013\nName: count, dtype: int64\n\n\n\n# NULL check\nwf.count_nulls(df)\n\nColumn                              NULL Count    % of Rows\n------------------------------------------------------------\nmax_glu_serum                           57,241       96.81%\nA1Cresult                               49,468       83.66%"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#medicines",
    "href": "Workfiles/Report_2_Workfile.html#medicines",
    "title": "Import",
    "section": "Medicines",
    "text": "Medicines\n\nCheckpoint 3\n\n# checkpoint 3\n# df_copy_3 = wf.checkpoint(df, existing_copy=globals().get('df_copy_7'))\n\n\n#df = df_copy_3.copy(deep=True)\n\nAlthough we could use all medicines in this study, let’s restrict to whether there is a medicine and whether it’s been changed.\n\n# lots of medicines\ndf.iloc[:,23:46]\n\n\n\n\n\n\n\n\nmetformin\nrepaglinide\nnateglinide\nchlorpropamide\nglimepiride\nacetohexamide\nglipizide\nglyburide\ntolbutamide\npioglitazone\nrosiglitazone\nacarbose\nmiglitol\ntroglitazone\ntolazamide\nexamide\ncitoglipton\ninsulin\nglyburide-metformin\nglipizide-metformin\nglimepiride-pioglitazone\nmetformin-rosiglitazone\nmetformin-pioglitazone\n\n\n\n\n20446\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\n\n\n20737\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\n\n\n20824\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n21083\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\n\n\n23668\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n101760\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nDown\nNo\nNo\nNo\nNo\nNo\n\n\n101761\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nDown\nNo\nNo\nNo\nNo\nNo\n\n\n101762\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\n\n\n101763\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nDown\nNo\nNo\nNo\nNo\nNo\n\n\n101764\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\n\n\n\n\n59129 rows × 23 columns\n\n\n\n\nmeds = df.columns[23:46].to_list()\n\n\ndf.drop(meds,axis=1,inplace=True)\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nencounter_id\npatient_nbr\nrace\ngender\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nmax_glu_serum\nA1Cresult\nchange\ndiabetesMed\nreadmit_early\nreadmit_at_all\n\n\n\n\n20446\n72091308\n20123568\nCaucasian\nFemale\n[70-80)\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\nNaN\nNaN\nCh\nYes\n0\n0\n\n\n20737\n72848634\n20377854\nCaucasian\nFemale\n[60-70)\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nV56\n403\n599\n6\nNaN\nNaN\nNo\nYes\n0\n0\n\n\n20824\n73062156\n20408121\nCaucasian\nFemale\n[90-100)\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\nNaN\nNaN\nNo\nYes\n0\n0\n\n\n\n\n\n\n\n\n\nA1C Result\n&gt;7 or &gt;8 indicates high A1C\n\ndf['A1Cresult'].value_counts()\n\nA1Cresult\n&gt;8      4146\nNorm    3146\n&gt;7      2369\nName: count, dtype: int64\n\n\n\ndf['high_a1c'] = df['A1Cresult'].isin(['&gt;7','&gt;8']).astype(int)\n\n\n\nMax Glucose Serum\n&gt;200 or &gt;300 indicate high glucose\n\ndf['max_glu_serum'].value_counts()\n\nmax_glu_serum\nNorm    922\n&gt;300    506\n&gt;200    460\nName: count, dtype: int64\n\n\n\ndf['glucose_high'] = df['max_glu_serum'].isin(['&gt;200', '&gt;300']).astype(int)\n\n\ncols = ['A1Cresult','max_glu_serum']\n\n\ndf.drop(cols,axis=1,inplace=True)\n\n\ndf.shape\n\n(59129, 27)"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#encoding",
    "href": "Workfiles/Report_2_Workfile.html#encoding",
    "title": "Import",
    "section": "Encoding",
    "text": "Encoding\n\nCheckpoint 4\n\n# checkpoint 4\n# df_copy_4 = wf.checkpoint(df, existing_copy=globals().get('df_copy_7'))\n\n\n# df = df_copy_4.copy(deep=True)\n\n\ndf['change'].value_counts()\n\nchange\nCh    30249\nNo    28880\nName: count, dtype: int64\n\n\n\ndf = wf.binary_labeler(data=df, data_cols=['change'], one_var='Ch', zero_var='No')\n\n\ndf['change_bin'].value_counts()\n\nchange_bin\n1    30249\n0    28880\nName: count, dtype: int64\n\n\n\ndf['diabetesMed'].value_counts()\n\ndiabetesMed\nYes    46960\nNo     12169\nName: count, dtype: int64\n\n\n\ndf = wf.binary_labeler(data=df,data_cols=['diabetesMed'], one_var='Yes', zero_var='No')\n\n\ndf['diabetesMed_bin'].value_counts()\n\ndiabetesMed_bin\n1    46960\n0    12169\nName: count, dtype: int64\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nencounter_id\npatient_nbr\nrace\ngender\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nchange\ndiabetesMed\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange_bin\ndiabetesMed_bin\n\n\n\n\n20446\n72091308\n20123568\nCaucasian\nFemale\n[70-80)\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\nCh\nYes\n0\n0\n0\n0\n1\n1\n\n\n20737\n72848634\n20377854\nCaucasian\nFemale\n[60-70)\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nV56\n403\n599\n6\nNo\nYes\n0\n0\n0\n0\n0\n1\n\n\n20824\n73062156\n20408121\nCaucasian\nFemale\n[90-100)\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\nNo\nYes\n0\n0\n0\n0\n0\n1\n\n\n21083\n73731852\n20542797\nCaucasian\nMale\n[70-80)\n1\n2\n7\n10\nMC\nInternalMedicine\n68\n1\n18\n0\n0\n0\n682\n427\n276\n6\nCh\nYes\n0\n0\n0\n0\n1\n1\n\n\n23668\n80820942\n20514150\nCaucasian\nFemale\n[60-70)\n2\n1\n1\n4\nMC\nUnknown\n33\n0\n11\n0\n0\n0\n250.02\n428\n401\n4\nCh\nYes\n1\n1\n0\n0\n1\n1\n\n\n\n\n\n\n\n\ndf = wf.binary_labeler(data=df,data_cols=['gender'],one_var='Female',zero_var='Male')\n\n\ndf['gender_bin'].value_counts()\n\ngender_bin\n1    31922\n0    27207\nName: count, dtype: int64\n\n\n\ndf.diag_1.value_counts(dropna=False)\n\ndiag_1\n428    3894\n414    3000\n486    2193\n786    2120\n410    1858\n       ... \n98        1\n990       1\n746       1\n381       1\n147       1\nName: count, Length: 656, dtype: int64\n\n\n\ndf.diag_1.value_counts(dropna=False)[150]\n\nnp.int64(54)\n\n\n\ndf.diag_2.value_counts(dropna=False)\n\ndiag_2\n276       4286\n428       3636\n250       3083\n427       3032\n401       2045\n          ... \n474          1\n250.91       1\n843          1\n320          1\n927          1\nName: count, Length: 695, dtype: int64\n\n\n\ndf.diag_3.value_counts(dropna=False)\n\ndiag_3\n250    6460\n401    4612\n276    3410\n428    2664\n427    2356\n       ... \n14        1\n750       1\n370       1\n671       1\n971       1\nName: count, Length: 730, dtype: int64\n\n\n\ndf.diag_1.isna().sum()\n\nnp.int64(0)\n\n\n\ndf.diag_2.isna().sum()\n\nnp.int64(0)\n\n\n\ndf.diag_3.isna().sum()\n\nnp.int64(0)\n\n\n\nfor col in ['diag_1', 'diag_2', 'diag_3']:\n    df = wf.group_rare_categories(df, column=col, threshold=100)\n\n\ndf.diag_1.value_counts(dropna=False)\n\ndiag_1\nOther    9425\n428      3894\n414      3000\n486      2193\n786      2120\n         ... \n540       105\n821       104\n721       104\n411       102\n444       100\nName: count, Length: 96, dtype: int64\n\n\n\ndf.diag_2.value_counts(dropna=False)\n\ndiag_2\nOther    8802\n276      4286\n428      3636\n250      3083\n427      3032\n         ... \n396       106\n458       104\n728       103\n348       102\n492       101\nName: count, Length: 94, dtype: int64\n\n\n\ndf.diag_3.value_counts(dropna=False)\n\ndiag_3\nOther     9041\n250       6460\n401       4612\n276       3410\n428       2664\n          ... \nV42        109\n410        108\n453        108\n250.92     102\n715        100\nName: count, Length: 97, dtype: int64\n\n\n\ncols = ['change','diabetesMed','gender']\n\n\ndf.drop(cols,axis=1,inplace=True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nencounter_id\npatient_nbr\nrace\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange_bin\ndiabetesMed_bin\ngender_bin\n\n\n\n\n20446\n72091308\n20123568\nCaucasian\n[70-80)\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\n0\n0\n0\n0\n1\n1\n1\n\n\n20737\n72848634\n20377854\nCaucasian\n[60-70)\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nOther\n403\n599\n6\n0\n0\n0\n0\n0\n1\n1\n\n\n20824\n73062156\n20408121\nCaucasian\n[90-100)\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\n0\n0\n0\n0\n0\n1\n1\n\n\n21083\n73731852\n20542797\nCaucasian\n[70-80)\n1\n2\n7\n10\nMC\nInternalMedicine\n68\n1\n18\n0\n0\n0\n682\n427\n276\n6\n0\n0\n0\n0\n1\n1\n0\n\n\n23668\n80820942\n20514150\nCaucasian\n[60-70)\n2\n1\n1\n4\nMC\nUnknown\n33\n0\n11\n0\n0\n0\n250.02\n428\n401\n4\n1\n1\n0\n0\n1\n1\n1\n\n\n\n\n\n\n\n\ndf.rename({'change_bin':'change','diabetesMed_bin':'on_meds','gender_bin':'gender'},axis=1,inplace=True)"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#repeat-patients",
    "href": "Workfiles/Report_2_Workfile.html#repeat-patients",
    "title": "Import",
    "section": "Repeat Patients",
    "text": "Repeat Patients\nHow often does a patient repeatedly visit?\n\ndf['patient_nbr'].value_counts()\n\npatient_nbr\n23460264    3\n42908625    3\n38049066    3\n99186975    3\n44051940    3\n           ..\n52005600    1\n25282791    1\n94219614    1\n43747200    1\n23863752    1\nName: count, Length: 37705, dtype: int64\n\n\n\nlen(df)/df['encounter_id'].nunique()\n\n\ndf.drop('encounter_id',axis=1,inplace=True)\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\npatient_nbr\nrace\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange\non_meds\ngender\n\n\n\n\n20446\n20123568\nCaucasian\n[70-80)\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\n0\n0\n0\n0\n1\n1\n1\n\n\n20737\n20377854\nCaucasian\n[60-70)\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nOther\n403\n599\n6\n0\n0\n0\n0\n0\n1\n1\n\n\n20824\n20408121\nCaucasian\n[90-100)\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n\n\n\n\n# Step 1: Count visits per patient\npatient_visits = df['patient_nbr'].value_counts()\n\n# Step 2: Convert to a DataFrame for analysis\nvisit_df = pd.DataFrame({'patient_nbr': patient_visits.index, 'visit_count': patient_visits.values})\n\n# Step 3: Plot the boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(y=visit_df['visit_count'], color='skyblue')\nplt.title('Number of Hospital Visits per Patient')\nplt.ylabel('Visit Count')\nplt.grid(True, axis='y')\nplt.show()\n\n# Step 4: Calculate IQR (Interquartile Range)\nQ1 = visit_df['visit_count'].quantile(0.25)\nQ3 = visit_df['visit_count'].quantile(0.75)\nIQR = Q3 - Q1\n\nprint(f\"Q1 (25th percentile): {Q1}\")\nprint(f\"Q3 (75th percentile): {Q3}\")\nprint(f\"IQR (Interquartile Range): {IQR}\")\n\n\n\n\n\n\n\n\nQ1 (25th percentile): 1.0\nQ3 (75th percentile): 2.0\nIQR (Interquartile Range): 1.0\n\n\n\nvisit_df['visit_count'].mean()\n\nnp.float64(1.4884206816694356)\n\n\nMean and Median are pretty close, but we see generally most patients only visit the hospital 1-2 times.\n\n# first checking if they would have multiple visits without readmission, which yes, but not always\ndf[df['patient_nbr'].isin(visit_df[(visit_df['visit_count']&gt;1)]['patient_nbr'])]['readmit_at_all'].value_counts()\n\nreadmit_at_all\n1    22248\n0     7496\nName: count, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\n# Step 1: Count visits per patient\npatient_visits = df['patient_nbr'].value_counts()\n\n# Step 2: Count how many patients had 1, 2, 3... visits\nvisit_distribution = patient_visits.value_counts().sort_index()\n\n# Step 3: Plot it\nplt.figure(figsize=(10, 6))\nplt.plot(visit_distribution.index, visit_distribution.values, marker='o', color='purple')\nplt.title('Number of Patients by Hospital Visit Frequency')\nplt.xlabel('Number of Visits')\nplt.ylabel('Number of Patients')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nvisit_df['visit_count'].value_counts()\n\nvisit_count\n1     29385\n2      6205\n3      2115\n4       938\n5       476\n6       241\n7       131\n8        80\n9        49\n10       27\n11       19\n12       16\n13        9\n15        7\n14        5\n18        5\n20        4\n19        4\n17        3\n16        3\n39        1\n23        1\n22        1\n21        1\nName: count, dtype: int64\n\n\n\ndf[(df['patient_nbr'].isin(visit_df[visit_df['visit_count']==3]['patient_nbr'].sample(1)))]\n\n\n\n\n\n\n\n\npatient_nbr\nrace\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange\non_meds\ngender\n\n\n\n\n46170\n28199475\nCaucasian\n[80-90)\n5\n3\n1\n4\nMC\nUnknown\n34\n0\n19\n0\n0\n0\n250.8\n780\nOther\n9\n0\n1\n0\n0\n1\n1\n0\n\n\n91890\n28199475\nCaucasian\n[80-90)\n1\n3\n7\n3\nMC\nUnknown\n57\n0\n22\n0\n0\n0\nOther\nOther\nOther\n9\n0\n1\n1\n0\n1\n1\n0\n\n\n99767\n28199475\nCaucasian\n[80-90)\n1\n3\n7\n4\nMC\nUnknown\n56\n0\n17\n0\n0\n1\n410\n428\n584\n9\n0\n1\n0\n0\n1\n1\n0\n\n\n\n\n\n\n\n\ndf = df[(df['patient_nbr'].isin(visit_df[visit_df['visit_count']&lt;=3]['patient_nbr']))]\n\n\nvisit_df[visit_df['visit_count']&lt;=3]['visit_count'].value_counts()\n\nvisit_count\n1    29385\n2     6205\n3     2115\nName: count, dtype: int64\n\n\n\nlen(df)\n\n48140\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\npatient_nbr\nrace\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange\non_meds\ngender\n\n\n\n\n20446\n20123568\nCaucasian\n[70-80)\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\n0\n0\n0\n0\n1\n1\n1\n\n\n20737\n20377854\nCaucasian\n[60-70)\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nOther\n403\n599\n6\n0\n0\n0\n0\n0\n1\n1\n\n\n20824\n20408121\nCaucasian\n[90-100)\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\n0\n0\n0\n0\n0\n1\n1\n\n\n21083\n20542797\nCaucasian\n[70-80)\n1\n2\n7\n10\nMC\nInternalMedicine\n68\n1\n18\n0\n0\n0\n682\n427\n276\n6\n0\n0\n0\n0\n1\n1\n0\n\n\n23668\n20514150\nCaucasian\n[60-70)\n2\n1\n1\n4\nMC\nUnknown\n33\n0\n11\n0\n0\n0\n250.02\n428\n401\n4\n1\n1\n0\n0\n1\n1\n1\n\n\n\n\n\n\n\nBecause of how patient visits are, when the time comes for train-test-split, I will do it on patient_nbr rather than arbitrary rows, so the same patient doesn’t end up in both the training and test data."
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#age",
    "href": "Workfiles/Report_2_Workfile.html#age",
    "title": "Import",
    "section": "Age",
    "text": "Age\n\n# checkpoint 6\n# df_copy_6 = wf.checkpoint(df, existing_copy=globals().get('df_copy_6'))\n\n\n# df = df_copy_6_copy(deep=True)\n\nEncoding age as an ordinal feature\n\ndf['age'].value_counts()\n\nage\n[70-80)     12416\n[60-70)     10590\n[80-90)      8927\n[50-60)      7898\n[40-50)      4296\n[90-100)     1609\n[30-40)      1555\n[20-30)       676\n[10-20)       167\n[0-10)          6\nName: count, dtype: int64\n\n\n\nage_map = {\n    '[0-10)': 0,\n    '[10-20)': 1,\n    '[20-30)': 2,\n    '[30-40)': 3,\n    '[40-50)': 4,\n    '[50-60)': 5,\n    '[60-70)': 6,\n    '[70-80)': 7,\n    '[80-90)': 8,\n    '[90-100)': 9\n}\n\ndf.loc[:, 'age_ord'] = df['age'].map(age_map)\n\n\ndf.drop('age',axis=1,inplace=True)\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\npatient_nbr\nrace\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange\non_meds\ngender\nage_ord\n\n\n\n\n20446\n20123568\nCaucasian\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\n0\n0\n0\n0\n1\n1\n1\n7\n\n\n20737\n20377854\nCaucasian\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nOther\n403\n599\n6\n0\n0\n0\n0\n0\n1\n1\n6\n\n\n20824\n20408121\nCaucasian\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\n0\n0\n0\n0\n0\n1\n1\n9"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#race",
    "href": "Workfiles/Report_2_Workfile.html#race",
    "title": "Import",
    "section": "Race",
    "text": "Race\n\n# checkpoint 7\n# df_copy_7 = wf.checkpoint(df, existing_copy=globals().get('df_copy_7'))\n\n\n# df = df_copy_7.copy(deep=True)\n\n\ndf = pd.get_dummies(df, columns=['race'])\n\n\ndf.reset_index(inplace=True,drop=True)\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\npatient_nbr\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange\non_meds\ngender\nage_ord\nrace_AfricanAmerican\nrace_Caucasian\nrace_Hispanic\nrace_Other\n\n\n\n\n0\n20123568\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\n0\n0\n0\n0\n1\n1\n1\n7\nFalse\nTrue\nFalse\nFalse\n\n\n1\n20377854\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nOther\n403\n599\n6\n0\n0\n0\n0\n0\n1\n1\n6\nFalse\nTrue\nFalse\nFalse\n\n\n2\n20408121\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\n0\n0\n0\n0\n0\n1\n1\n9\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 48140 entries, 0 to 48139\nData columns (total 29 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   patient_nbr               48140 non-null  int64 \n 1   admission_type_id         48140 non-null  int64 \n 2   discharge_disposition_id  48140 non-null  int64 \n 3   admission_source_id       48140 non-null  int64 \n 4   time_in_hospital          48140 non-null  int64 \n 5   payer_code                48140 non-null  object\n 6   medical_specialty         48140 non-null  object\n 7   num_lab_procedures        48140 non-null  int64 \n 8   num_procedures            48140 non-null  int64 \n 9   num_medications           48140 non-null  int64 \n 10  number_outpatient         48140 non-null  int64 \n 11  number_emergency          48140 non-null  int64 \n 12  number_inpatient          48140 non-null  int64 \n 13  diag_1                    48140 non-null  object\n 14  diag_2                    48140 non-null  object\n 15  diag_3                    48140 non-null  object\n 16  number_diagnoses          48140 non-null  int64 \n 17  readmit_early             48140 non-null  int64 \n 18  readmit_at_all            48140 non-null  int64 \n 19  high_a1c                  48140 non-null  int64 \n 20  glucose_high              48140 non-null  int64 \n 21  change                    48140 non-null  int64 \n 22  on_meds                   48140 non-null  int64 \n 23  gender                    48140 non-null  int64 \n 24  age_ord                   48140 non-null  int64 \n 25  race_AfricanAmerican      48140 non-null  bool  \n 26  race_Caucasian            48140 non-null  bool  \n 27  race_Hispanic             48140 non-null  bool  \n 28  race_Other                48140 non-null  bool  \ndtypes: bool(4), int64(20), object(5)\nmemory usage: 9.4+ MB\n\n\n\nrace_cols = [col for col in df.columns if col.startswith(\"race_\")]\ndf[race_cols] = df[race_cols].astype(int)"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#ids",
    "href": "Workfiles/Report_2_Workfile.html#ids",
    "title": "Import",
    "section": "IDs",
    "text": "IDs\n\ndf.head()\n\n\n\n\n\n\n\n\nencounter_id\npatient_nbr\nrace\ngender\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nmax_glu_serum\nA1Cresult\nmetformin\nrepaglinide\nnateglinide\nchlorpropamide\nglimepiride\nacetohexamide\nglipizide\nglyburide\ntolbutamide\npioglitazone\nrosiglitazone\nacarbose\nmiglitol\ntroglitazone\ntolazamide\nexamide\ncitoglipton\ninsulin\nglyburide-metformin\nglipizide-metformin\nglimepiride-pioglitazone\nmetformin-rosiglitazone\nmetformin-pioglitazone\nchange\ndiabetesMed\nreadmitted\nreadmit_early\nreadmit_at_all\nadmission_type\n\n\n\n\n20446\n72091308\n20123568\nCaucasian\nFemale\n[70-80)\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\nNone\nNone\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nCh\nYes\nNO\n0\n0\nNaN\n\n\n20737\n72848634\n20377854\nCaucasian\nFemale\n[60-70)\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nV56\n403\n599\n6\nNone\nNone\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nYes\nNO\n0\n0\nNaN\n\n\n20824\n73062156\n20408121\nCaucasian\nFemale\n[90-100)\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\nNone\nNone\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nYes\nNO\n0\n0\nNaN\n\n\n21083\n73731852\n20542797\nCaucasian\nMale\n[70-80)\n1\n2\n7\n10\nMC\nInternalMedicine\n68\n1\n18\n0\n0\n0\n682\n427\n276\n6\nNone\nNone\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nCh\nYes\nNO\n0\n0\nNaN\n\n\n23668\n80820942\n20514150\nCaucasian\nFemale\n[60-70)\n2\n1\n1\n4\nMC\nUnknown\n33\n0\n11\n0\n0\n0\n250.02\n428\n401\n4\nNone\nNone\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\nCh\nYes\n&lt;30\n1\n1\nNaN\n\n\n\n\n\n\n\n\nids.head()\n\n\n\n\n\n\n\n\nadmission_type_id\ndescription\n\n\n\n\n0\n1\nEmergency\n\n\n1\n2\nUrgent\n\n\n2\n3\nElective\n\n\n3\n4\nNewborn\n\n\n4\n5\nNot Available\n\n\n\n\n\n\n\n\nids[:60]\n\n\n\n\n\n\n\n\nadmission_type_id\ndescription\n\n\n\n\n0\n1\nEmergency\n\n\n1\n2\nUrgent\n\n\n2\n3\nElective\n\n\n3\n4\nNewborn\n\n\n4\n5\nNot Available\n\n\n5\n6\nNaN\n\n\n6\n7\nTrauma Center\n\n\n7\n8\nNot Mapped\n\n\n8\nNaN\nNaN\n\n\n9\ndischarge_disposition_id\ndescription\n\n\n10\n1\nDischarged to home\n\n\n11\n2\nDischarged/transferred to another short term hospital\n\n\n12\n3\nDischarged/transferred to SNF\n\n\n13\n4\nDischarged/transferred to ICF\n\n\n14\n5\nDischarged/transferred to another type of inpatient care institution\n\n\n15\n6\nDischarged/transferred to home with home health service\n\n\n16\n7\nLeft AMA\n\n\n17\n8\nDischarged/transferred to home under care of Home IV provider\n\n\n18\n9\nAdmitted as an inpatient to this hospital\n\n\n19\n10\nNeonate discharged to another hospital for neonatal aftercare\n\n\n20\n11\nExpired\n\n\n21\n12\nStill patient or expected to return for outpatient services\n\n\n22\n13\nHospice / home\n\n\n23\n14\nHospice / medical facility\n\n\n24\n15\nDischarged/transferred within this institution to Medicare approved swing bed\n\n\n25\n16\nDischarged/transferred/referred another institution for outpatient services\n\n\n26\n17\nDischarged/transferred/referred to this institution for outpatient services\n\n\n27\n18\nNaN\n\n\n28\n19\nExpired at home. Medicaid only, hospice.\n\n\n29\n20\nExpired in a medical facility. Medicaid only, hospice.\n\n\n30\n21\nExpired, place unknown. Medicaid only, hospice.\n\n\n31\n22\nDischarged/transferred to another rehab fac including rehab units of a hospital .\n\n\n32\n23\nDischarged/transferred to a long term care hospital.\n\n\n33\n24\nDischarged/transferred to a nursing facility certified under Medicaid but not certified under Medicare.\n\n\n34\n25\nNot Mapped\n\n\n35\n26\nUnknown/Invalid\n\n\n36\n30\nDischarged/transferred to another Type of Health Care Institution not Defined Elsewhere\n\n\n37\n27\nDischarged/transferred to a federal health care facility.\n\n\n38\n28\nDischarged/transferred/referred to a psychiatric hospital of psychiatric distinct part unit of a hospital\n\n\n39\n29\nDischarged/transferred to a Critical Access Hospital (CAH).\n\n\n40\nNaN\nNaN\n\n\n41\nadmission_source_id\ndescription\n\n\n42\n1\nPhysician Referral\n\n\n43\n2\nClinic Referral\n\n\n44\n3\nHMO Referral\n\n\n45\n4\nTransfer from a hospital\n\n\n46\n5\nTransfer from a Skilled Nursing Facility (SNF)\n\n\n47\n6\nTransfer from another health care facility\n\n\n48\n7\nEmergency Room\n\n\n49\n8\nCourt/Law Enforcement\n\n\n50\n9\nNot Available\n\n\n51\n10\nTransfer from critial access hospital\n\n\n52\n11\nNormal Delivery\n\n\n53\n12\nPremature Delivery\n\n\n54\n13\nSick Baby\n\n\n55\n14\nExtramural Birth\n\n\n56\n15\nNot Available\n\n\n57\n17\nNaN\n\n\n58\n18\nTransfer From Another Home Health Agency\n\n\n59\n19\nReadmission to Same Home Health Agency\n\n\n\n\n\n\n\n\ndef parse_stacked_mapping(raw_ids):\n    # Find header row indices\n    header_rows = raw_ids[raw_ids[1] == 'description'].index.tolist()\n    header_rows.append(len(raw_ids))  # Add end for slicing\n\n    mappings = {}\n\n    # Iterate over blocks\n    for i in range(len(header_rows) - 1):\n        start = header_rows[i] + 1\n        end = header_rows[i+1]\n\n        var_name = raw_ids.iloc[header_rows[i], 0]  # e.g. 'admission_type_id'\n        sub_df = raw_ids.iloc[start:end].dropna()\n\n        # Rename columns\n        sub_df.columns = ['code', 'description']\n        sub_df = sub_df.astype({'code': int})  # convert code from float to int\n        mappings[var_name] = dict(zip(sub_df['code'], sub_df['description']))\n\n    return mappings\n\n\n# Parse it into a dict of dicts\nid_maps = parse_stacked_mapping(ids)\n\n\n# Now you can map like:\ndf['admission_type'] = df['admission_type_id'].map(id_maps['admission_type_id'])\ndf['discharge_disposition'] = df['discharge_disposition_id'].map(id_maps['discharge_disposition_id'])\ndf['admission_source'] = df['admission_source_id'].map(id_maps['admission_source_id'])\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nencounter_id\npatient_nbr\nrace\ngender\nage\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\npayer_code\nmedical_specialty\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\ndiag_1\ndiag_2\ndiag_3\nnumber_diagnoses\nmax_glu_serum\nA1Cresult\nmetformin\nrepaglinide\nnateglinide\nchlorpropamide\nglimepiride\nacetohexamide\nglipizide\nglyburide\ntolbutamide\npioglitazone\nrosiglitazone\nacarbose\nmiglitol\ntroglitazone\ntolazamide\nexamide\ncitoglipton\ninsulin\nglyburide-metformin\nglipizide-metformin\nglimepiride-pioglitazone\nmetformin-rosiglitazone\nmetformin-pioglitazone\nchange\ndiabetesMed\nreadmitted\nreadmit_early\nreadmit_at_all\nadmission_type\ndischarge_disposition\nadmission_source\n\n\n\n\n20446\n72091308\n20123568\nCaucasian\nFemale\n[70-80)\n1\n22\n7\n7\nMC\nOther\n58\n2\n15\n0\n0\n0\n821\n276\n285\n9\nNone\nNone\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nUp\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nCh\nYes\nNO\n0\n0\nEmergency\nDischarged/transferred to another rehab fac including rehab units of a hospital .\nEmergency Room\n\n\n20737\n72848634\n20377854\nCaucasian\nFemale\n[60-70)\n2\n1\n1\n3\nMC\nOther\n59\n3\n11\n0\n0\n0\nV56\n403\n599\n6\nNone\nNone\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nYes\nNO\n0\n0\nUrgent\nDischarged to home\nPhysician Referral\n\n\n20824\n73062156\n20408121\nCaucasian\nFemale\n[90-100)\n1\n1\n7\n4\nMC\nEmergency/Trauma\n56\n1\n9\n0\n0\n0\n532\n428\n535\n6\nNone\nNone\nNo\nNo\nNo\nNo\nSteady\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nYes\nNO\n0\n0\nEmergency\nDischarged to home\nEmergency Room"
  },
  {
    "objectID": "Research_Report_2.html#read-in-data",
    "href": "Research_Report_2.html#read-in-data",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "4.1 Read in Data",
    "text": "4.1 Read in Data\nData Dimensions:\n\n\nCode\n# load in encoded data\ndata &lt;- read_csv('data/RR2/modeling_data_encoded.csv')\n\ndim(data) # 48,140 x 89\n\n\n[1] 48140    90\n\n\nTrain-Test-Split\n\n\nCode\n# remove the secondary y-target\ndata &lt;- data |&gt;\n  dplyr::select(-readmit_early)\n\ny &lt;- data$readmit_at_all\nX &lt;- data |&gt; dplyr::select(-readmit_at_all)\n\nset.seed(42)\n\n# TRAIN TEST SPLIT #\n\n# need to separate by patient_nbr so same patients don't end up in train and target\npatient_ids &lt;- unique(data$patient_nbr)\ntrain_ids &lt;- sample(patient_ids, size = 0.8 * length(patient_ids))\n\ntrain_data &lt;- data |&gt; filter(patient_nbr %in% train_ids)\ntest_data &lt;- data |&gt; filter(!(patient_nbr %in% train_ids))\n\n# drop patient_nbr\ntrain_data &lt;- train_data |&gt; dplyr::select(-patient_nbr)\ntest_data &lt;- test_data |&gt; dplyr::select(-patient_nbr)\n\n\nThe below sub-sections will just contain the code-folds for the models with their respective Confusion Matrices and accuracy scores. The models will eventually be used in the FairStacks ensemble."
  },
  {
    "objectID": "Research_Report_2.html#regular-stack",
    "href": "Research_Report_2.html#regular-stack",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "5.1 Regular Stack",
    "text": "5.1 Regular Stack\nFirst, let’s see what a regular stacked model would look like without the fairness penalty.\n\n\nCode\n# regular stack\n\n# Convert SVM margin to probabilities\nsigmoid &lt;- function(x) {\n  1 / (1 + exp(-x))\n}\nprob_pred_svm &lt;- sigmoid(margin_scores)\n\n# Tree probabilities\ntree_probs &lt;- predict(tree_model, test_data, type = \"prob\")\nprob_pred_tree &lt;- tree_probs[, \"1\"]\n\n# Combine all model probabilities using correct variable names\nensemble_train &lt;- data.frame(\n  nb           = nb_prob,\n  lda          = lda_pred$posterior[, 2],\n  svm          = as.numeric(prob_pred_svm),\n  tree         = as.numeric(prob_pred_tree),\n  logreg_plain = as.numeric(prob_pred_plain),\n  logreg_ridge = as.numeric(prob_pred_ridge),\n  logreg_lasso = as.numeric(prob_pred_lasso)\n)\n\n# Normalize each prediction (min-max scaling)\nensemble_scaled &lt;- ensemble_train %&gt;%\n  mutate(across(everything(), ~ (. - min(.)) / (max(.) - min(.))))\n\nF_train &lt;- as.matrix(ensemble_scaled)\nn &lt;- nrow(F_train)\nk &lt;- ncol(F_train)\n\n# Response: convert 0/1 → -1/+1\ny_train_ens &lt;- ifelse(test_data$readmit_at_all == 1, 1, -1)\n\n# CVXR optimization\nW &lt;- Variable(k)\nb &lt;- Variable(1)\nmargin &lt;- F_train %*% W + b\n\nlog_loss &lt;- sum(logistic(- y_train_ens * margin)) / n\nlambda &lt;- 0.0001\nobjective &lt;- Minimize(log_loss + lambda * sum_squares(W))\nproblem &lt;- Problem(objective)\nresult &lt;- solve(problem)\n\n# Predict and evaluate\nweights &lt;- result$getValue(W)\nintercept &lt;- result$getValue(b)\n\nensemble_scores &lt;- F_train %*% weights + intercept\nfinal_pred &lt;- ifelse(ensemble_scores &gt;= 0, 1, 0)\n\ny_true &lt;- as.numeric(as.character(test_data$readmit_at_all))\nensemble_cm &lt;- confusionMatrix(as.factor(final_pred), as.factor(y_true))\n\n# Plot confusion matrix\nensemble_cm_df &lt;- as.data.frame(ensemble_cm$table)\n\nggplot(data = ensemble_cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 5) +\n  scale_fill_gradient(low = \"white\", high = \"purple\") +\n  labs(\n    title = \"Stacked Ensemble Confusion Matrix\",\n    x = \"Actual Label\",\n    y = \"Predicted Label\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Ensemble Accuracy:\", round(ensemble_cm$overall[\"Accuracy\"], 4), \"\\n\")\n\n\nEnsemble Accuracy: 0.6381 \n\n\n\n\nCode\n# Define model names in the same order as your ensemble_train\nmodel_names &lt;- c(\n  \"NaiveBayes\", \"LDA\", \"SVM\", \"DecisionTree\",\n  \"LogReg_Plain\", \"LogReg_Ridge\", \"LogReg_Lasso\"\n)\n\n# Extract weights from regular stack optimization\nregular_weights &lt;- as.numeric(result$getValue(W))\n\n# Build tibble\nweights_df_regular &lt;- tibble(\n  Model = factor(model_names, levels = model_names),\n  Weight = regular_weights\n)\n\n# Plot\nggplot(weights_df_regular, aes(x = Model, y = Weight)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Regular Stack Ensemble Weights\",\n    x = NULL,\n    y = \"Model Weight\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Research_Report_2.html#fairstacks",
    "href": "Research_Report_2.html#fairstacks",
    "title": "Ensemble Learning Techniques for Fair Classification",
    "section": "5.2 FairStacks",
    "text": "5.2 FairStacks\nNow comparing the regular stack to a FairStack:\n\n\nCode\n# Response vector\ny_train_ens &lt;- ifelse(test_data$readmit_at_all == 1, 1, -1)\n\n\n# Estimate bias per model (Demographic Parity)\n\n# protected attribute\nprotected_attr &lt;- test_data$race_AfricanAmerican\n\n# demographic disparity per model\nget_ddp &lt;- function(preds, protected) {\n  mean(preds[protected == 1]) - mean(preds[protected == 0])\n}\n\nbias_vec &lt;- sapply(ensemble_scaled, get_ddp, protected = protected_attr)\nbias_vec &lt;- matrix(bias_vec, nrow = length(bias_vec), ncol = 1)\n\n# FairStacks Optimization\n\nn &lt;- nrow(F_train)\nk &lt;- ncol(F_train)\n\nW &lt;- Variable(k)\nb &lt;- Variable(1)\n\nmargin &lt;- F_train %*% W + b\nlog_loss &lt;- sum(logistic(- y_train_ens * margin)) / n\n\nlambda &lt;- 0.0001  # regularization on weights\ngamma &lt;- 10       # fairness penalty\n\nfair_penalty &lt;- abs(t(bias_vec) %*% W)\n\nobjective &lt;- Minimize(log_loss + lambda * sum_squares(W) + gamma * fair_penalty)\nproblem &lt;- Problem(objective)\nresult &lt;- solve(problem)\n\n# Predict and evaluate\n\nweights &lt;- result$getValue(W)\nintercept &lt;- result$getValue(b)\n\nensemble_scores &lt;- F_train %*% weights + intercept\nfinal_pred &lt;- ifelse(ensemble_scores &gt;= 0, 1, 0)\n\ny_true &lt;- as.numeric(as.character(test_data$readmit_at_all))\nensemble_cm &lt;- confusionMatrix(as.factor(final_pred), as.factor(y_true))\n\n# Plot confusion matrix\nensemble_cm_df &lt;- as.data.frame(ensemble_cm$table)\n\nggplot(data = ensemble_cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 5) +\n  scale_fill_gradient(low = \"white\", high = \"purple\") +\n  labs(\n    title = \"FairStacks Ensemble Confusion Matrix\",\n    x = \"Actual Label\",\n    y = \"Predicted Label\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ncat(\"FairStacks Accuracy:\", round(ensemble_cm$overall[\"Accuracy\"], 4), \"\\n\")\n\n\nFairStacks Accuracy: 0.6331 \n\n\n\n\nCode\n# Define model names — in same order as used in the ensemble\nmodel_names &lt;- c(\n  \"NaiveBayes\", \"LDA\", \"SVM\", \"DecisionTree\",\n  \"LogReg_Plain\", \"LogReg_Ridge\", \"LogReg_Lasso\"\n)\n\n# Extract weights from CVXR solution\nfair_weights &lt;- as.numeric(result$getValue(W))  # shape: [k x 1]\n\n# Create data frame\nweights_df &lt;- tibble(\n  Model = factor(model_names, levels = model_names),\n  Weight = fair_weights\n)\n\n# Plot\nggplot(weights_df, aes(x = Model, y = Weight)) +\n  geom_bar(stat = \"identity\", fill = \"darkgreen\") +\n  theme_minimal() +\n  labs(\n    title = \"FairStacks Ensemble Weights\",\n    x = NULL,\n    y = \"Model Weight\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#target-class-imbalance",
    "href": "Workfiles/Report_2_Workfile.html#target-class-imbalance",
    "title": "Import",
    "section": "Target Class Imbalance",
    "text": "Target Class Imbalance\n\ny1 = df['readmit_early']\ny2 = df['readmit_at_all']\n\n\ny1.value_counts(normalize=True)\n\nreadmit_early\n0    0.922019\n1    0.077981\nName: proportion, dtype: float64\n\n\n\ny2.value_counts(normalize=True)\n\nreadmit_at_all\n0    0.626485\n1    0.373515\nName: proportion, dtype: float64"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#multicollinearity",
    "href": "Workfiles/Report_2_Workfile.html#multicollinearity",
    "title": "Import",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\n# statsmodels libraries\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n\nX = df.select_dtypes(include=['int64','int32','uint8']).drop(columns=['readmit_early'])\nX = add_constant(X)\n\n\nnon_numeric = ['patient_nbr','admission_type_id',\n               'discharge_disposition_id','admission_source_id'\n               ]\nX.drop(non_numeric,axis=1,inplace=True)\n\n\nvif_data = pd.DataFrame({\n    \"Feature\": X.columns,\n    \"VIF\": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n})\n\nprint(vif_data)\n\n               Feature        VIF\n0                const  36.337888\n1     time_in_hospital   1.405461\n2   num_lab_procedures   1.241400\n3       num_procedures   1.195813\n4      num_medications   1.641633\n5    number_outpatient   1.021472\n6     number_emergency   1.047705\n7     number_inpatient   1.059117\n8     number_diagnoses   1.156847\n9       readmit_at_all   1.027945\n10            high_a1c   1.077821\n11        glucose_high   1.026322\n12              change   1.439457\n13             on_meds   1.385661\n14              gender   1.013340\n15             age_ord   1.088847\n\n\n\n# features vs log-odds\nfor col in ['age_ord','time_in_hospital','num_medications','number_inpatient']:\n    sns.boxplot(x='readmit_early',y=col,data=df)\n    plt.title(f'{col} vs Readmission')\n    plt.show()"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#scaling-features",
    "href": "Workfiles/Report_2_Workfile.html#scaling-features",
    "title": "Import",
    "section": "Scaling Features",
    "text": "Scaling Features\n\n# Normalize features\nfrom sklearn.preprocessing import StandardScaler\n\n\nfeatures_to_scale = [\n    'age_ord', 'time_in_hospital', 'num_lab_procedures', 'num_medications',\n    'number_outpatient', 'number_emergency', 'number_inpatient',\n    'number_diagnoses'\n]\n\nscaler = StandardScaler()\ndf[features_to_scale] = scaler.fit_transform(df[features_to_scale])"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#encoding-categorical-columns",
    "href": "Workfiles/Report_2_Workfile.html#encoding-categorical-columns",
    "title": "Import",
    "section": "Encoding Categorical Columns",
    "text": "Encoding Categorical Columns\n\ndf.select_dtypes(include='object')\n\n\n\n\n\n\n\n\npayer_code\nmedical_specialty\ndiag_1\ndiag_2\ndiag_3\n\n\n\n\n0\nMC\nOther\n821\n276\n285\n\n\n1\nMC\nOther\nOther\n403\n599\n\n\n2\nMC\nEmergency/Trauma\n532\n428\n535\n\n\n3\nMC\nInternalMedicine\n682\n427\n276\n\n\n4\nMC\nUnknown\n250.02\n428\n401\n\n\n...\n...\n...\n...\n...\n...\n\n\n48135\nMC\nUnknown\nOther\n8\n304\n\n\n48136\nMC\nUnknown\n435\n784\n250\n\n\n48137\nMC\nUnknown\n250.13\nOther\n458\n\n\n48138\nMC\nUnknown\n560\n276\n787\n\n\n48139\nMC\nUnknown\n38\nOther\n296\n\n\n\n\n48140 rows × 5 columns\n\n\n\n\ncategorical_cols = ['payer_code', 'medical_specialty', 'diag_1', 'diag_2', 'diag_3']\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n\n\nfor col in categorical_cols:\n    df = wf.collapse_rare_categories(df, col)\n\n\ndf_encoded.columns[df_encoded.columns.str.startswith('diag_')]\n\nIndex(['diag_1_162', 'diag_1_197', 'diag_1_198', 'diag_1_250', 'diag_1_250.02',\n       'diag_1_250.1', 'diag_1_250.11', 'diag_1_250.12', 'diag_1_250.13',\n       'diag_1_250.4',\n       ...\n       'diag_3_E849', 'diag_3_E878', 'diag_3_Other', 'diag_3_V10',\n       'diag_3_V12', 'diag_3_V15', 'diag_3_V42', 'diag_3_V43', 'diag_3_V45',\n       'diag_3_V58'],\n      dtype='object', length=284)\n\n\n\ndf_encoded['diag_1_162']\n\n0        False\n1        False\n2        False\n3        False\n4        False\n         ...  \n48135    False\n48136    False\n48137    False\n48138    False\n48139    False\nName: diag_1_162, Length: 48140, dtype: bool\n\n\n\ndummy_cols = df_encoded.columns[df_encoded.dtypes == 'bool']\n\ndf_encoded[dummy_cols] = df_encoded[dummy_cols].astype(int)"
  },
  {
    "objectID": "Workfiles/Report_2_Workfile.html#correlation",
    "href": "Workfiles/Report_2_Workfile.html#correlation",
    "title": "Import",
    "section": "Correlation",
    "text": "Correlation\n\n# Select only numeric columns\ndf_numeric = df.select_dtypes(include='number')\n\n# Create the correlation matrix\ncorr = df_numeric.corr()\n\nplt.figure(figsize=(14, 8), facecolor=\"#2e2e2e\")\n\n# Draw heatmap with green color map\nsns.heatmap(\n    corr,\n    cmap=\"YlGn\",                   # green color scheme\n    annot=False,\n    linewidths=0.5,\n    square=True,\n    cbar_kws={\"shrink\": 0.8},\n    xticklabels=True,\n    yticklabels=True\n)\n\n# Improve label appearance\nplt.xticks(rotation=90, color='white')\nplt.yticks(rotation=0, color='white')\nplt.title(\"Correlation Heatmap\", color='white')\nplt.gca().patch.set_facecolor('#2e2e2e')  # set axis background\nplt.grid(False)\n\n# Show\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Get numeric features\ndf_numeric = df.select_dtypes(include='number')\n\ndf_numeric.drop(['patient_nbr','admission_type_id','admission_source_id',\n                 'discharge_disposition_id','readmit_early'\n                 ],axis=1,inplace=True\n                 )\n\n# Compute correlation matrix\ncorr_matrix = df_numeric.corr()\n\n# Extract correlations with the target\ntarget = 'readmit_at_all'\ncorr_with_target = corr_matrix[target].drop(target)  # exclude self-correlation\n\n# Sort by absolute correlation\ncorr_table = corr_with_target.reindex(corr_with_target.abs().sort_values(ascending=False).index)\n\ncorr_df = pd.DataFrame({\n    'Feature': corr_table.index,\n    'Correlation': corr_table.values,\n    'AbsCorrelation': corr_table.abs().values\n})\n\n\n# Display\nprint(corr_df.to_string(index=False))\n\n             Feature  Correlation  AbsCorrelation\n    number_diagnoses     0.100887        0.100887\n    number_inpatient     0.094070        0.094070\n    number_emergency     0.069979        0.069979\n             age_ord     0.060875        0.060875\n   number_outpatient     0.059428        0.059428\n             on_meds     0.051651        0.051651\n      race_Caucasian     0.049902        0.049902\n              change     0.038971        0.038971\n     num_medications     0.038761        0.038761\nrace_AfricanAmerican    -0.038742        0.038742\n  num_lab_procedures     0.038280        0.038280\n    time_in_hospital     0.036561        0.036561\n       race_Hispanic    -0.024821        0.024821\n      num_procedures    -0.022897        0.022897\n          race_Other    -0.016992        0.016992\n        glucose_high    -0.010632        0.010632\n              gender     0.006951        0.006951\n            high_a1c     0.001505        0.001505\n\n\n\ndiag_cols = [col for col in df_encoded.columns if col.startswith('diag_')]\n\ncorr_diag = df_encoded[diag_cols + ['readmit_at_all']].corr()\ncorr_with_target = corr_diag['readmit_at_all'].drop('readmit_at_all')\n\ncorr_diag_sorted = corr_with_target.reindex(corr_with_target.abs().sort_values(ascending=False).index)\n\n# Optional: format as a DataFrame for display\nimport pandas as pd\n\ncorr_diag_df = pd.DataFrame({\n    'Feature': corr_diag_sorted.index,\n    'Correlation': corr_diag_sorted.values,\n    'AbsCorrelation': corr_diag_sorted.abs().values\n})\n\nprint(corr_diag_df.to_string(index=False))\n\n      Feature  Correlation  AbsCorrelation\n   diag_1_428     0.054958        0.054958\n   diag_3_250    -0.038016        0.038016\n diag_1_Other    -0.037473        0.037473\n   diag_2_648    -0.036062        0.036062\n   diag_1_278    -0.035914        0.035914\n   diag_2_250    -0.035631        0.035631\n   diag_2_401    -0.035441        0.035441\n   diag_3_401    -0.031773        0.031773\n   diag_2_428     0.029019        0.029019\n   diag_1_491     0.027409        0.027409\n   diag_2_491     0.025989        0.025989\n   diag_2_585     0.025549        0.025549\n   diag_3_403     0.024995        0.024995\n   diag_2_403     0.024723        0.024723\n   diag_3_427     0.024204        0.024204\n   diag_1_493     0.024000        0.024000\n   diag_2_707     0.023997        0.023997\n   diag_3_496     0.022047        0.022047\n   diag_3_428     0.021531        0.021531\n   diag_3_682     0.021311        0.021311\n   diag_3_585     0.020476        0.020476\n   diag_1_540    -0.020205        0.020205\n   diag_1_V58     0.020143        0.020143\n   diag_3_272    -0.019209        0.019209\n   diag_3_424     0.018949        0.018949\n   diag_2_496     0.017087        0.017087\n   diag_1_574    -0.016993        0.016993\n   diag_2_682     0.016414        0.016414\n   diag_2_348    -0.015820        0.015820\n   diag_2_305    -0.015195        0.015195\n   diag_2_272    -0.014868        0.014868\n diag_3_250.6     0.014810        0.014810\ndiag_1_250.82     0.014805        0.014805\n diag_2_Other    -0.014784        0.014784\n   diag_1_486     0.014395        0.014395\ndiag_2_250.02     0.014364        0.014364\n   diag_1_722    -0.014354        0.014354\n   diag_2_518    -0.013824        0.013824\n diag_1_250.6     0.013714        0.013714\n   diag_3_536     0.013548        0.013548\n   diag_3_280     0.013472        0.013472\n   diag_2_427     0.012989        0.012989\n   diag_1_250    -0.012976        0.012976\n   diag_1_440     0.012750        0.012750\n   diag_1_404     0.012685        0.012685\n   diag_3_707     0.012576        0.012576\n diag_3_Other    -0.012522        0.012522\ndiag_3_250.02     0.012503        0.012503\n   diag_1_198    -0.012395        0.012395\n   diag_2_530    -0.012350        0.012350\n   diag_2_425     0.012186        0.012186\n   diag_1_162    -0.012182        0.012182\n   diag_1_996     0.011883        0.011883\n   diag_1_410    -0.011852        0.011852\n   diag_1_403     0.011483        0.011483\n   diag_2_263    -0.011478        0.011478\n diag_1_250.8     0.011323        0.011323\n   diag_2_788    -0.010933        0.010933\n  diag_3_E849    -0.010911        0.010911\n diag_3_250.4     0.010889        0.010889\n   diag_1_648    -0.010840        0.010840\n   diag_3_599     0.010788        0.010788\n   diag_2_996     0.010765        0.010765\n   diag_2_560    -0.010706        0.010706\n   diag_1_197    -0.010480        0.010480\n   diag_1_518    -0.010306        0.010306\n   diag_2_998    -0.010158        0.010158\n   diag_1_733     0.010133        0.010133\ndiag_1_250.12    -0.010013        0.010013\n   diag_2_571     0.009824        0.009824\n   diag_2_787    -0.009618        0.009618\n   diag_3_305    -0.009586        0.009586\n   diag_1_789     0.009519        0.009519\n   diag_3_V10    -0.009385        0.009385\n     diag_2_8     0.009328        0.009328\n   diag_2_V85    -0.009298        0.009298\n   diag_3_560    -0.009270        0.009270\n    diag_3_41    -0.009185        0.009185\n   diag_1_280     0.009180        0.009180\n   diag_2_535     0.009150        0.009150\ndiag_1_250.13    -0.009132        0.009132\n   diag_1_824    -0.009064        0.009064\n   diag_2_507    -0.009058        0.009058\n   diag_1_531     0.009042        0.009042\n   diag_2_574    -0.008935        0.008935\n   diag_3_425     0.008873        0.008873\n   diag_1_415    -0.008738        0.008738\n   diag_3_244    -0.008738        0.008738\n   diag_2_733     0.008647        0.008647\n diag_3_250.8     0.008566        0.008566\n   diag_1_507    -0.008429        0.008429\n   diag_3_276    -0.008408        0.008408\n diag_1_250.4     0.008390        0.008390\n   diag_3_416     0.008390        0.008390\n   diag_1_715    -0.008210        0.008210\n   diag_2_162    -0.008168        0.008168\n   diag_2_396     0.008011        0.008011\n   diag_2_997    -0.007828        0.007828\n   diag_2_486    -0.007766        0.007766\n   diag_2_278    -0.007759        0.007759\n   diag_2_536     0.007724        0.007724\n   diag_3_396     0.007646        0.007646\n   diag_3_198    -0.007628        0.007628\n   diag_3_V43    -0.007575        0.007575\n   diag_1_532    -0.007566        0.007566\n   diag_2_196     0.007395        0.007395\n   diag_1_285     0.007286        0.007286\n    diag_2_38    -0.007275        0.007275\n   diag_3_410    -0.007242        0.007242\n   diag_3_V12     0.007117        0.007117\n   diag_2_789    -0.007100        0.007100\n   diag_1_590    -0.007051        0.007051\n   diag_2_790     0.007046        0.007046\n   diag_3_278    -0.007045        0.007045\n   diag_2_197    -0.006971        0.006971\n   diag_3_493     0.006897        0.006897\n   diag_1_562     0.006851        0.006851\n   diag_1_530    -0.006718        0.006718\n diag_2_250.8     0.006671        0.006671\n   diag_1_331    -0.006629        0.006629\n   diag_2_786    -0.006581        0.006581\n   diag_3_997    -0.006537        0.006537\n   diag_3_491     0.006533        0.006533\n   diag_1_730     0.006525        0.006525\n   diag_1_724    -0.006522        0.006522\n   diag_1_569     0.006468        0.006468\ndiag_1_250.02    -0.006444        0.006444\n   diag_1_786    -0.006440        0.006440\n    diag_2_70    -0.006418        0.006418\n   diag_1_276     0.006411        0.006411\n   diag_2_730     0.006365        0.006365\n   diag_1_557    -0.006311        0.006311\n   diag_2_296    -0.006257        0.006257\n   diag_3_402    -0.006205        0.006205\n   diag_3_V58    -0.006090        0.006090\n   diag_3_799     0.006068        0.006068\n   diag_1_560    -0.006003        0.006003\n   diag_2_780    -0.005917        0.005917\ndiag_3_250.01     0.005907        0.005907\n   diag_3_715    -0.005900        0.005900\n   diag_2_303     0.005831        0.005831\n   diag_2_342     0.005828        0.005828\n   diag_3_V45     0.005813        0.005813\n   diag_1_453    -0.005801        0.005801\n   diag_2_785    -0.005777        0.005777\n diag_2_250.6     0.005691        0.005691\n   diag_2_411     0.005676        0.005676\n   diag_3_304     0.005675        0.005675\n   diag_2_304     0.005630        0.005630\n   diag_3_V42     0.005542        0.005542\n   diag_3_162     0.005511        0.005511\n diag_1_250.7     0.005501        0.005501\n   diag_2_295     0.005440        0.005440\n   diag_2_424     0.005379        0.005379\n   diag_2_453    -0.005348        0.005348\n   diag_1_599     0.005343        0.005343\n   diag_2_434    -0.005339        0.005339\n  diag_3_E878     0.005318        0.005318\n   diag_1_584    -0.005316        0.005316\n   diag_3_295     0.005228        0.005228\n   diag_3_V15    -0.005217        0.005217\n   diag_3_287    -0.005215        0.005215\n   diag_2_202     0.005174        0.005174\n   diag_1_552    -0.005018        0.005018\n   diag_3_263    -0.005018        0.005018\n   diag_1_V57     0.004968        0.004968\n   diag_3_584     0.004963        0.004963\n   diag_3_578     0.004951        0.004951\n   diag_2_410     0.004846        0.004846\n     diag_3_8    -0.004820        0.004820\n   diag_1_578     0.004802        0.004802\n   diag_3_780    -0.004794        0.004794\n   diag_3_730     0.004756        0.004756\n   diag_3_785    -0.004713        0.004713\n   diag_2_426    -0.004657        0.004657\n   diag_3_303     0.004577        0.004577\n   diag_2_V42     0.004577        0.004577\n   diag_3_296     0.004567        0.004567\n   diag_1_707     0.004459        0.004459\n   diag_1_805     0.004419        0.004419\n   diag_1_821    -0.004339        0.004339\n   diag_1_820    -0.004261        0.004261\n   diag_1_998     0.004259        0.004259\n   diag_2_728    -0.004241        0.004241\n   diag_1_466    -0.004204        0.004204\n   diag_3_357     0.004177        0.004177\n   diag_1_414    -0.004136        0.004136\n   diag_3_197    -0.004134        0.004134\n   diag_2_511     0.004074        0.004074\n   diag_3_294     0.004071        0.004071\n   diag_2_438     0.003928        0.003928\ndiag_2_250.82     0.003895        0.003895\n   diag_2_799    -0.003842        0.003842\ndiag_2_250.01    -0.003724        0.003724\n   diag_1_721    -0.003713        0.003713\n   diag_1_402     0.003667        0.003667\n   diag_3_440     0.003576        0.003576\n   diag_1_571     0.003480        0.003480\n   diag_3_285     0.003372        0.003372\n   diag_2_V45    -0.003346        0.003346\n   diag_3_453     0.003288        0.003288\n     diag_1_8     0.003267        0.003267\ndiag_3_250.92     0.003252        0.003252\n   diag_3_331    -0.003242        0.003242\n   diag_2_591    -0.003198        0.003198\n   diag_3_413     0.003170        0.003170\n   diag_1_444     0.003103        0.003103\n   diag_1_558    -0.003076        0.003076\n   diag_3_571     0.003035        0.003035\ndiag_2_250.92     0.002976        0.002976\n   diag_3_438     0.002939        0.002939\n   diag_2_287    -0.002913        0.002913\n   diag_1_411    -0.002884        0.002884\n   diag_3_458    -0.002870        0.002870\n   diag_1_997    -0.002856        0.002856\n   diag_3_426     0.002849        0.002849\n   diag_2_578    -0.002807        0.002807\n   diag_2_493    -0.002713        0.002713\n   diag_1_434     0.002688        0.002688\n    diag_3_70     0.002620        0.002620\n   diag_3_593    -0.002602        0.002602\n   diag_1_295    -0.002536        0.002536\n   diag_1_592    -0.002531        0.002531\n   diag_3_412    -0.002476        0.002476\n   diag_1_482     0.002381        0.002381\n   diag_1_575    -0.002280        0.002280\n   diag_2_458    -0.002242        0.002242\n   diag_2_198    -0.002194        0.002194\n   diag_2_577    -0.002099        0.002099\n   diag_2_276     0.002083        0.002083\n   diag_3_787     0.002077        0.002077\n   diag_1_572     0.001987        0.001987\n   diag_2_285     0.001965        0.001965\n   diag_1_787     0.001950        0.001950\n   diag_1_426     0.001885        0.001885\n   diag_3_535     0.001876        0.001876\n    diag_2_41    -0.001851        0.001851\n   diag_2_402    -0.001808        0.001808\n   diag_3_411    -0.001788        0.001788\n   diag_3_511    -0.001759        0.001759\n   diag_2_440     0.001670        0.001670\n    diag_1_38    -0.001659        0.001659\n   diag_1_780     0.001638        0.001638\n   diag_1_427     0.001489        0.001489\n   diag_2_599     0.001433        0.001433\n   diag_3_443     0.001375        0.001375\ndiag_1_250.11     0.001372        0.001372\n   diag_1_435     0.001346        0.001346\n   diag_1_296    -0.001338        0.001338\n   diag_3_530    -0.001334        0.001334\n diag_1_250.1    -0.001288        0.001288\n   diag_3_788     0.001285        0.001285\n   diag_2_404     0.001280        0.001280\n   diag_3_300    -0.001252        0.001252\n   diag_2_584     0.001250        0.001250\n   diag_3_790    -0.001216        0.001216\n   diag_2_784    -0.001179        0.001179\n   diag_3_995    -0.001127        0.001127\n   diag_3_786     0.001109        0.001109\n   diag_3_562     0.001100        0.001100\n   diag_2_V58    -0.001037        0.001037\n   diag_2_492     0.001033        0.001033\n   diag_1_401    -0.001006        0.001006\n   diag_2_280     0.000991        0.000991\n   diag_1_812    -0.000983        0.000983\n   diag_3_998     0.000900        0.000900\n   diag_3_733     0.000599        0.000599\n   diag_3_486     0.000591        0.000591\n   diag_2_569    -0.000576        0.000576\n   diag_3_311     0.000510        0.000510\n   diag_3_996     0.000483        0.000483\n   diag_2_413    -0.000405        0.000405\n   diag_1_535    -0.000392        0.000392\n   diag_3_784     0.000370        0.000370\n   diag_1_511    -0.000326        0.000326\n   diag_1_600    -0.000308        0.000308\n   diag_3_414     0.000296        0.000296\n   diag_1_577    -0.000234        0.000234\n   diag_1_458     0.000198        0.000198\n   diag_2_414    -0.000178        0.000178\n   diag_1_433    -0.000173        0.000173\n   diag_3_789     0.000159        0.000159\n   diag_1_682    -0.000083        0.000083\n   diag_3_518     0.000067        0.000067\n\n\n\ndiag_cols_to_drop = corr_diag_df.sort_values(by='AbsCorrelation',ascending=False)['Feature'][50:].to_list()\n\n\ndf_encoded.drop(diag_cols_to_drop,axis=1,inplace=True)\n\n\ndf_encoded\n\n\n\n\n\n\n\n\npatient_nbr\nadmission_type_id\ndischarge_disposition_id\nadmission_source_id\ntime_in_hospital\nnum_lab_procedures\nnum_procedures\nnum_medications\nnumber_outpatient\nnumber_emergency\nnumber_inpatient\nnumber_diagnoses\nreadmit_early\nreadmit_at_all\nhigh_a1c\nglucose_high\nchange\non_meds\ngender\nage_ord\nrace_AfricanAmerican\nrace_Caucasian\nrace_Hispanic\nrace_Other\npayer_code_CM\npayer_code_CP\npayer_code_HM\npayer_code_MC\npayer_code_MD\npayer_code_OG\npayer_code_Other\npayer_code_SP\npayer_code_UN\nmedical_specialty_Emergency/Trauma\nmedical_specialty_Family/GeneralPractice\nmedical_specialty_InternalMedicine\nmedical_specialty_Other\nmedical_specialty_Radiologist\nmedical_specialty_Surgery-General\nmedical_specialty_Unknown\ndiag_1_198\ndiag_1_250\ndiag_1_250.6\ndiag_1_250.82\ndiag_1_278\ndiag_1_404\ndiag_1_428\ndiag_1_440\ndiag_1_486\ndiag_1_491\ndiag_1_493\ndiag_1_540\ndiag_1_574\ndiag_1_722\ndiag_1_Other\ndiag_1_V58\ndiag_2_250\ndiag_2_250.02\ndiag_2_272\ndiag_2_305\ndiag_2_348\ndiag_2_401\ndiag_2_403\ndiag_2_427\ndiag_2_428\ndiag_2_491\ndiag_2_496\ndiag_2_518\ndiag_2_530\ndiag_2_585\ndiag_2_648\ndiag_2_682\ndiag_2_707\ndiag_2_Other\ndiag_3_250\ndiag_3_250.02\ndiag_3_250.6\ndiag_3_272\ndiag_3_280\ndiag_3_401\ndiag_3_403\ndiag_3_424\ndiag_3_427\ndiag_3_428\ndiag_3_496\ndiag_3_536\ndiag_3_585\ndiag_3_682\ndiag_3_707\ndiag_3_Other\n\n\n\n\n0\n20123568\n1\n22\n7\n0.962946\n0.797493\n2\n-0.123201\n-0.310277\n-0.248788\n-0.474576\n0.767218\n0\n0\n0\n0\n1\n1\n1\n0.508428\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n20377854\n2\n1\n1\n-0.420760\n0.846804\n3\n-0.620416\n-0.310277\n-0.248788\n-0.474576\n-0.881346\n0\n0\n0\n0\n0\n1\n1\n-0.139541\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n20408121\n1\n1\n7\n-0.074833\n0.698871\n1\n-0.869023\n-0.310277\n-0.248788\n-0.474576\n-0.881346\n0\n0\n0\n0\n0\n1\n1\n1.804364\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n20542797\n1\n2\n7\n2.000726\n1.290605\n1\n0.249710\n-0.310277\n-0.248788\n-0.474576\n-0.881346\n0\n0\n0\n0\n1\n1\n0\n0.508428\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n20514150\n2\n1\n1\n-0.074833\n-0.435286\n0\n-0.620416\n-0.310277\n-0.248788\n-0.474576\n-1.980388\n1\n1\n0\n0\n1\n1\n1\n-0.139541\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48135\n120975314\n1\n1\n7\n0.271093\n1.685095\n1\n0.746924\n-0.310277\n1.251108\n-0.474576\n0.767218\n0\n0\n0\n0\n1\n1\n1\n1.156396\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n48136\n86472243\n1\n1\n7\n-1.112613\n-2.013244\n0\n-0.123201\n1.978175\n-0.248788\n-0.474576\n-0.331825\n0\n0\n0\n0\n1\n1\n0\n1.156396\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n48137\n100162476\n1\n3\n7\n-0.420760\n0.452315\n0\n0.001103\n-0.310277\n-0.248788\n-0.474576\n0.767218\n0\n1\n1\n0\n1\n1\n0\n0.508428\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n48138\n74694222\n1\n4\n5\n0.271093\n-0.435286\n3\n0.249710\n-0.310277\n-0.248788\n0.877585\n0.767218\n0\n0\n0\n0\n0\n1\n1\n1.156396\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n48139\n41088789\n1\n1\n7\n-1.112613\n0.550937\n0\n-0.869023\n0.452540\n-0.248788\n-0.474576\n2.965303\n0\n0\n0\n0\n1\n1\n0\n0.508428\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n48140 rows × 90 columns"
  }
]