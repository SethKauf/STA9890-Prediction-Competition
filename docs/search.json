[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prediction Competition",
    "section": "",
    "text": "Landing Page for Prediction Competition\n\n\n\n\n\n\n\nLast Updated: Sunday 03 23, 2025 at 17:08PM"
  },
  {
    "objectID": "Research_Report_1.html#bias-and-variance",
    "href": "Research_Report_1.html#bias-and-variance",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.1 Bias and Variance",
    "text": "2.1 Bias and Variance\nIn modeling, Bias and Variance each play an important role in building a usable model that can predict some outcome \\(y\\). Both terms combined give us a general error term which in turn can be used to tell us how good a model actually is.\n\n\n\n\nBias and Variance Tradeoff via Wikipedia\n\n\n\nBias is the “unavoidable model error”, that is it looks at the difference between the ground truth and the predicted values of our model. Variance on the other hand, is the squared difference between the predicted values and the mean of the predicted values."
  },
  {
    "objectID": "Research_Report_1.html#mse",
    "href": "Research_Report_1.html#mse",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.2 MSE",
    "text": "2.2 MSE\nA common error term is the Mean Squared Error or MSE. MSE in the 2D-linear world is defined as:\n\\(MSE=\\frac{(y-\\hat y)^2}{n}\\)\nOr y-true less y-predicted squared over the number of samples n. In the world of Machine Learning though, we usually use matrices to fit our models. For example, in OLS regression, we are searching for a matrix-version of a “line of best fit”, which looks like:\n\\(y = X\\beta + \\epsilon\\)\nIn Matrix Notation, the Expected Value breaks out as:\n\\(E[MSE]=Bias^2+Variance \\space (+Irreducible \\space Noise)\\)\nUsing the terminology from the previous section:\n\\(Bias^2=E[(f(X)-E[\\hat f(X)])^2]\\)\n$Variance = E[(f(X)-Ef(X)])^2]\nWhile the Irreducible Noise refers to the fact that, by the nature of how modelling and prediction work, there will simply always be factors that can’t be accounted for. It is denoted by:\n\\(\\sigma^2=E[\\epsilon^2]\\)\nWhere \\(\\epsilon\\) is the error term.\nThe full breakdown of the Expected MSE formula is as follows\\(^1\\):\n\\(MSE(\\hat \\theta)=E_{\\theta}[(\\hat \\theta - \\theta)^2]\\) \\(^2\\)\nBreaking this out further:\n\\(=E_{\\theta}[(\\hat \\theta - E_\\theta[\\hat \\theta]+E_\\theta[\\hat \\theta]-\\theta)^2]\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2 + 2(\\hat \\theta - E_\\theta [\\hat \\theta])(E_\\theta [\\hat \\theta] - \\hat \\theta)+(E_\\theta[\\hat \\theta - \\theta)^2]\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+E_\\theta [2(\\hat \\theta - E_\\theta[\\hat \\theta])(E_\\theta[\\hat \\theta]-\\theta)]+E_\\theta[(E_\\theta[\\hat \\theta] - \\theta)^2]\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+2(E_\\theta[\\hat \\theta]-\\theta) E_\\theta[\\hat \\theta - E_\\theta[\\hat \\theta]] + (E_\\theta[\\hat \\theta] - \\theta)^2\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+2(E_\\theta[\\hat \\theta]-\\theta)E_\\theta[\\hat \\theta - E_\\theta[\\hat \\theta]]+(E_\\theta[\\hat \\theta] - \\theta)^2\\)\n\\(=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]+(E_\\theta[\\hat \\theta] - \\theta)^2\\)\nRemembering the Bias and Variance formulas:\n\\(Bias^2=E[(f(X)-E[\\hat f(X)])^2]=E_\\theta[(\\hat \\theta - E_\\theta[\\hat \\theta])^2]\\)\n\\(Variance=E[(\\hat f(X)-E[\\hat f(X)])^2]=(E_\\theta[\\hat \\theta] - \\theta)^2\\)\nWith irreducible error term \\(E[\\epsilon]\\)"
  },
  {
    "objectID": "Research_Report_1.html#ols-regression-and-blue",
    "href": "Research_Report_1.html#ols-regression-and-blue",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.3 OLS Regression and BLUE",
    "text": "2.3 OLS Regression and BLUE\nOLS Regression is a simple linear model that tries to fit a line to the data that minimizes the Residual Sum of Squares error term. It is generally considered to fit the criteria of BLUE, that is Best Unbiased Linear Estimator. It is the Best because it has the lowest variance among all other linear estimators, it is Linear because that is the model-type, it is Unbiased because there is no difference between the ground truth and the prediction, specifically the expected value of the predicted parameter is equal to the parameter in the DGP, or \\(E[\\hat \\theta]=\\theta\\), and it is an estimator because it estimates \\(\\theta^2\\). However, there are exceptions to this BLUE property."
  },
  {
    "objectID": "Research_Report_1.html#dgp-and-misspecification",
    "href": "Research_Report_1.html#dgp-and-misspecification",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "2.4 DGP and Misspecification",
    "text": "2.4 DGP and Misspecification\nA Data Generating Process or DGP is the function of the world that generates data. It is the “ground truth” upon which we try to model the data we do have. Everything discussed until now all is built on-top of this idea that the real-world has unknowns and we can only do our best to try and understand and even predict those unknowns.\nIn the context of OLS Regression, the faults for OLS stem from the fact that it is only a BLUE model-type when the underlying DGP is linear. However, what happens if the real DGP isn’t \\(y=X\\beta+\\epsilon\\), rather it’s \\(y=X^2 +\\epsilon\\) a quadratic equation. This would be model misspecification and would cause issues in trying to estimate parameters our DGP. We can test for whether OLS would be a good model type by looking at various factors. First is an expectation of Homoscedasticity in the data, that is \\(Var(\\epsilon_i|X)=\\sigma^2\\) or that the residuals (that is, \\(e=y-\\hat y\\)) have a constant error variance. Running an OLS model on the Credit Card dataset from ISLR\\(^3\\) with a predicted variable of \\(y\\) being the credit card balance, it produces the following chart:\n\n\n\n\n\n\n\n\n\n\n\n\n Homoscedasticity of \\(\\hat y\\) from ISL Credit Data Set\n\nIf these were more evenly spread out around the mean, this would be a good indication that the OLS model would be the best to use. However, that is not the case here: we see somewhat of a checkmark shape that coalesces in the bottom-left.\nTo check for the unbiased property in the DGP, we can look towards Endogeneity, or that the errors are not affected by the input \\(X\\) variables: \\(E[\\epsilon|X]=0\\). Looking at the input X variables:\n\n\n\n\n\n\n\n\n\n\n\n\n Endogeneity of feature residuals from ISL Credit Data Set\n\nThe spread is not very even here, though it’s not extremely lopsided. These however show that we likely will have bias in our model if we try to use OLS."
  },
  {
    "objectID": "Research_Report_1.html#derivation-for-ols",
    "href": "Research_Report_1.html#derivation-for-ols",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.1 Derivation for OLS",
    "text": "3.1 Derivation for OLS\nOLS seeks to minimize some predictor variable \\(\\hat \\beta\\) to minimize the error function. To do this, we look at the error term that OLS is trying to minimize, then substitute it in to find a proper value for \\(\\hat \\beta\\):\n\\(RSS=\\Sigma^n_{i=1}=||\\epsilon||^2\\)\nSince \\(y=X\\beta+\\epsilon\\) we can rearrange the terms so \\(\\epsilon=y-X\\beta\\), expand the terms, then derive with respect to \\(\\beta\\):\n\\(RSS=||y-X||^2=(y-X\\beta)^T(y-X\\beta)\\)\n\\(\\frac{d}{d\\beta}[(y-X\\beta)^T(y-X\\beta)]=\\frac{d}{d\\beta}[y^Ty-2\\beta^T X^Ty+\\beta^TX^TX\\beta]\\)\nThe \\(y^T y\\) term is a constant while \\(\\frac{d}{d\\beta}[\\beta^T]=1\\) so we get the following and set it equal to 0 to minimize it:\n\\(-2X^Ty+2X^T X\\beta=0\\)\n\\(2X^TX\\beta=2X^T y\\)\n\\(\\beta = \\frac {X^T y} {X^T X}=(X^T X)^{-1} X^T y\\)"
  },
  {
    "objectID": "Research_Report_1.html#ols-gradient",
    "href": "Research_Report_1.html#ols-gradient",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.2 OLS Gradient",
    "text": "3.2 OLS Gradient\nGradient Descent refers to the process of iteratively fitting models to the gradient of a function is the vector field whose value at point p gives the direction and rate of the fastest increase\\(^4\\). In OLS terms, this translates to\n\\(\\beta^{(k+1)}=\\beta^{k}-c\\nabla\\mathcal{L}|_{\\beta=\\beta^{(k)}}\\) where \\(\\nabla\\mathcal{L}\\) is the loss function on the gradient.\nGradient Descent keeps going until the parameter and objective each converge, or: \\(\\beta^{(k+1)}\\approx\\beta^{(k)}\\) and \\(\\mathcal{L}(\\beta^{(k+1)})\\approx\\mathcal{L}(\\beta^{(k)})\\).\nThe setup is similar to the closed-form derivation from the previous step, it turns into:\n\\(\\beta^{(k+1)}=\\beta^k-c\\mathcal{L}|_{\\beta=\\beta^k}=\\beta^k-c(-2X^Ty+2X^TX\\beta)=\\beta^k-2c(X^TX\\beta^k-X^Ty)\\)\nPlotting it out, it looks like:\n\n\nConverged at iteration 293\n\n\n\n\n\n\n\n Sample Gradient Descent with convergence in OLS"
  },
  {
    "objectID": "Research_Report_2.html",
    "href": "Research_Report_2.html",
    "title": "Research Report 2",
    "section": "",
    "text": "STA 9890 – Research Report 2\nResearch Report 2 focuses on Ensemble Learning Techniques for Fair Classification."
  },
  {
    "objectID": "Research_Report_1.html#ols-weight-decay",
    "href": "Research_Report_1.html#ols-weight-decay",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.3 OLS Weight Decay",
    "text": "3.3 OLS Weight Decay\nTo try and make up for the issues of overfitting and large parameterization, OLS can implement a process known as weight decay. This adds a penalty term to the gradient formula to penalize large parameters and try to keep the model simple. The set-up is similar to before but with an added penalty term:\n\\(\\beta^{(k+1)}=\\beta^{k}-c\\nabla\\mathcal{L}|_{\\beta=\\beta^{(k)}}-\\mathcal{w}\\beta^{(k)}\\)\nPlugging this into the gradient derivative from the previous section, we get to this step:\n\\(=\\frac{\\partial}{\\partial \\beta}[y^Ty-2\\beta^TX^Ty+\\beta^TX^TX\\beta-\\mathcal{w}\\beta]\\)\nThe \\(y^Ty\\) term disappears since it’s a constant in this context and the \\(\\beta^T\\) terms set to \\(\\frac{\\partial}{\\partial \\beta}[\\beta^T]=1\\)\n\\(\\nabla\\mathcal{L}=-2X^Ty+2X^TX\\beta-\\mathcal{w}\\beta\\)\nPlugging this back into the original formula for \\(\\beta^{(k+1)}\\):\n\\(\\beta^{(k+1)}=\\beta^{(k)}-c(-2X^Ty+2X^TX\\beta-2\\mathcal{w}\\beta)\\)\nWhich finally equals:\n\\(\\beta^{(k+1)}=\\beta^{(k)}-2c(X^TX\\beta^{(k)}-X^Ty-\\mathcal{w}I)\\)\nAnd in simulation-land, we can see this will converge sooner than just regular Gradient Descent from the previous section:\n\n\nCode\n## Simulate Gradient Descent\n\n# Function\ngradient_descent_weight_decay &lt;- function(X, y, alpha = 0.01, lambda_val = 0.1, num_iterations = 1000, tol = 1e-6) {\n  # Add intercept column if not already present\n  if (!all(X[,1] == 1)) {\n    X &lt;- cbind(1, X)\n  }\n  \n  n &lt;- nrow(X)\n  k &lt;- ncol(X)\n  beta &lt;- rep(0, k)\n  loss_history &lt;- numeric()\n  \n  for (i in 1:num_iterations) {\n    # Compute gradient with weight decay\n    gradient &lt;- 2 * (t(X) %*% X %*% beta - t(X) %*% y + lambda_val * beta) / n\n    beta &lt;- (1 - 2 * alpha * lambda_val) * beta - alpha * gradient\n    \n    # Compute loss (MSE + L2 penalty)\n    loss &lt;- mean((y - X %*% beta)^2) + lambda_val * sum(beta^2)\n    loss_history[i] &lt;- loss\n    \n    # Convergence check\n    if (i &gt; 1 && abs(loss_history[i] - loss_history[i - 1]) &lt; tol) {\n      message(\"Converged at iteration \", i)\n      break\n    }\n  }\n  \n  list(beta = beta, loss_history = loss_history)\n}\n\n\n\n\nConverged at iteration 141\n\n\n\n\n\n\n\n\n\nSample Weight Decay and Gradient Descent with convergence in OLS"
  },
  {
    "objectID": "Research_Report_1.html#ridge-regression",
    "href": "Research_Report_1.html#ridge-regression",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "3.4 Ridge Regression",
    "text": "3.4 Ridge Regression\nRidge Regression is an \\(\\ell_p-Regression\\) function, specifically \\(\\ell_2\\). It applies a penalty term to the original OLS parameters, \\(\\lambda\\), to minimize the effects of large coefficients and overfitting.\n\\(\\ell_2 (\\beta) = ||y-X\\beta||^2+\\lambda ||\\beta||^2\\)\nBreaking this out then deriving it with respect to β:\n\\(\\ell_2 (\\beta) = (y-X\\beta)^T (y-X\\beta) + \\lambda \\beta^T \\beta = -2X^T y + 2X^T X + 2\\lambda \\beta\\)\nSetting this equal to 0 and solving for \\(\\beta\\):\n\\(-2X^Ty + 2X^T X \\beta + 2 \\lambda \\beta = 0\\)\nWhere \\(\\beta = (X^T X \\lambda I)^{-1} X^T y\\)\nDeriving the gradient term \\(\\beta^{k+1} = \\beta^k - c \\nabla L|_{\\beta = \\beta^k}=\\beta^k-2c(X^T y + X^T X \\beta^k - \\lambda I)\\)\nWhich is the same form as OLS with weight decay: \\(\\beta^k - 2c(X^T y + X^T X \\beta^k - wI)\\)"
  },
  {
    "objectID": "Research_Report_1.html",
    "href": "Research_Report_1.html",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "",
    "text": "Research Report 1 covers the Bias-Variance Trade off in a Linear Data Generating Process (DGP) and the BLUE-ness of Ordinary Least Squares (OLS) Regression. This will include the theoretical background, computation of the gradient descent and weight decay, and bias and variance under both a linear and non-linear DGP. Some basic statistical background is assumed for this report, e.g.: I won’t delve into detail on \\(Y=f(x)+\\epsilon\\), which will be treated as common knowledge."
  },
  {
    "objectID": "Research_Report_1.html#positing-a-linear-dgp-with-monte-carlo",
    "href": "Research_Report_1.html#positing-a-linear-dgp-with-monte-carlo",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "4.1 Positing a Linear DGP with Monte Carlo",
    "text": "4.1 Positing a Linear DGP with Monte Carlo\nIf we assume a Linear DGP of \\(y=X\\beta + \\epsilon\\), we can use Monte Carlo simulations to view bias and variance under these conditions.\n\n\nCode\n# Parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nbeta_true &lt;- c(5, 2)  # Intercept and slope\n\n# Initialize storage\nresults &lt;- data.frame()\n\n# Monte Carlo loop\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n  \n  for (sim in 1:num_simulations) {\n    # Generate training data\n    X &lt;- runif(n, -3, 3)\n    y &lt;- beta_true[1] + beta_true[2] * X + rnorm(n, 0, 1)\n    X_mat &lt;- cbind(1, X)  # Add intercept\n    \n    # Fit model\n    model &lt;- lm(y ~ X)\n    beta_hat &lt;- coef(model)\n    beta_estimates[sim, ] &lt;- beta_hat\n    \n    # In-sample MSE\n    y_pred &lt;- predict(model)\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n    \n    # Generate test set\n    X_test &lt;- runif(n, -3, 3)\n    y_test &lt;- beta_true[1] + beta_true[2] * X_test + rnorm(n, 0, 1)\n    test_df &lt;- data.frame(X = X_test)\n    \n    # Out-of-sample MSE\n    y_test_pred &lt;- predict(model, newdata = test_df)\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n  \n  # Compute bias and variance for β1 (slope)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta_true[2]\n  variance_beta1 &lt;- var(beta1_hat)\n  \n  # Store results\n  results &lt;- rbind(results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Plot Bias, Variance, and MSEs\npar(mfrow = c(2, 2))\n\n# Plot Bias\np1 &lt;- ggplot(results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size\", x = \"Sample Size (n)\", y = \"Bias\")\n\n# Plot Variance\np2 &lt;- ggplot(results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size\", x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-Sample MSE\np3 &lt;- ggplot(results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size\", x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-Sample MSE\np4 &lt;- ggplot(results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size\", x = \"Sample Size (n)\", y = \"MSE\")\n\n# Arrange in 2x2 grid\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nBias and Variance under a Linear DGP with Monte Carlo simulations"
  },
  {
    "objectID": "Research_Report_1.html#repeating-with-ridge-regression",
    "href": "Research_Report_1.html#repeating-with-ridge-regression",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "4.2 Repeating with Ridge Regression",
    "text": "4.2 Repeating with Ridge Regression\nLet’s repeat the Monte Carlo simulations using Ridge Regression to see the difference between it and OLS\n\n\nCode\n# Set simulation parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nbeta_true &lt;- c(5, 2)\nlambda_ridge &lt;- 1.0  # L2 regularization strength\n\n# Storage for results\nridge_results &lt;- data.frame()\n\n# Loop over sample sizes\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n\n  for (sim in 1:num_simulations) {\n    # Generate training data\n    X &lt;- runif(n, -3, 3)\n    y &lt;- beta_true[1] + beta_true[2] * X + rnorm(n)\n    X_mat &lt;- cbind(1, X)  # Add intercept column\n\n    # Fit Ridge regression (glmnet expects matrix input, no intercept term)\n    ridge_model &lt;- glmnet(x = X_mat, y = y, alpha = 0, lambda = lambda_ridge, intercept = FALSE, standardize = FALSE)\n    beta_hat &lt;- as.vector(coef(ridge_model, s = lambda_ridge))[-1]  # drop intercept row\n    beta_estimates[sim, ] &lt;- beta_hat\n\n    # In-sample prediction\n    y_pred &lt;- X_mat %*% beta_hat\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n\n    # Test set\n    X_test &lt;- runif(n, -3, 3)\n    y_test &lt;- beta_true[1] + beta_true[2] * X_test + rnorm(n)\n    X_test_mat &lt;- cbind(1, X_test)\n\n    # Out-of-sample prediction\n    y_test_pred &lt;- X_test_mat %*% beta_hat\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n\n  # Bias and variance of β1 (slope)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta_true[2]\n  variance_beta1 &lt;- var(beta1_hat)\n\n  # Save results\n  ridge_results &lt;- rbind(ridge_results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Bias\np1 &lt;- ggplot(ridge_results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"Bias\")\n\n# Variance\np2 &lt;- ggplot(ridge_results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-sample MSE\np3 &lt;- ggplot(ridge_results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-sample MSE\np4 &lt;- ggplot(ridge_results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size (Ridge)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Display all four plots in 2x2 layout\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n Bias and Variance under a Linear DGP with Monte Carlo simulations using Ridge Regression\n\nWhile we can see the bias is always a little worse in Ridge Regression, the variance takes a sharper decline; eventually we end up with a better out-of-sample MSE than we did with OLS."
  },
  {
    "objectID": "Research_Report_1.html#monte-carlo-simulation",
    "href": "Research_Report_1.html#monte-carlo-simulation",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "5.1 Monte Carlo Simulation",
    "text": "5.1 Monte Carlo Simulation\nSimilar to the previous section, here we will look at how bias and variance are affected when the DGP is non-linear, \\(y=X^2 \\beta + \\epsilon\\)\n\n\nCode\n# Parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nlambda_ridge &lt;- 1.0\nbeta &lt;- c(5, 2)  # Intercept and quadratic coefficient\n\n# Function to simulate non-linear DGP: y = β0 + β1 * x^2 + ε\ngenerate_nonlinear_dgp &lt;- function(n, beta = c(5, 2), noise_sd = 1.0) {\n  x &lt;- runif(n, -3, 3)\n  y &lt;- beta[1] + beta[2] * x^2 + rnorm(n, sd = noise_sd)\n  return(list(x = x, y = y))\n}\n\n# Storage for results\nridge_nonlinear_results &lt;- data.frame()\n\n# Simulation\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n\n  for (sim in 1:num_simulations) {\n    # Generate training data\n    train &lt;- generate_nonlinear_dgp(n)\n    x &lt;- train$x\n    y &lt;- train$y\n    X_quad &lt;- cbind(1, x^2)\n\n    # Fit Ridge regression\n    ridge_model &lt;- glmnet(x = X_quad, y = y, alpha = 0, lambda = lambda_ridge,\n                          intercept = FALSE, standardize = FALSE)\n    beta_hat &lt;- as.vector(coef(ridge_model, s = lambda_ridge))[-1]\n    beta_estimates[sim, ] &lt;- beta_hat\n\n    # In-sample MSE\n    y_pred &lt;- X_quad %*% beta_hat\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n\n    # Test set\n    test &lt;- generate_nonlinear_dgp(n)\n    x_test &lt;- test$x\n    y_test &lt;- test$y\n    X_test_quad &lt;- cbind(1, x_test^2)\n    y_test_pred &lt;- X_test_quad %*% beta_hat\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n\n  # Bias and variance for β1 (quadratic term)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta[2]\n  variance_beta1 &lt;- var(beta1_hat)\n\n  # Store results\n  ridge_nonlinear_results &lt;- rbind(ridge_nonlinear_results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Bias\np1 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Bias\")\n\n# Variance\np2 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-sample MSE\np3 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-sample MSE\np4 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Display 2x2 grid\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\nThe bias is now nearly constant as seen in the top-left, while the in-and-out of sample errors seem to still follow the trends we saw with the last two."
  },
  {
    "objectID": "Research_Report_1.html#monte-carlo-simulation-for-non-linear-dgp",
    "href": "Research_Report_1.html#monte-carlo-simulation-for-non-linear-dgp",
    "title": "Bias and Variance Tradeoff and BLUE",
    "section": "5.1 Monte Carlo Simulation for non-Linear DGP",
    "text": "5.1 Monte Carlo Simulation for non-Linear DGP\nSimilar to the previous section, here we will look at how bias and variance are affected when the DGP is non-linear, \\(y=X^2 \\beta + \\epsilon\\)\n\n\nCode\n# Parameters\nnum_simulations &lt;- 500\nsample_sizes &lt;- c(50, 100, 200)\nlambda_ridge &lt;- 1.0\nbeta &lt;- c(5, 2)  # Intercept and quadratic coefficient\n\n# Function to simulate non-linear DGP: y = β0 + β1 * x^2 + ε\ngenerate_nonlinear_dgp &lt;- function(n, beta = c(5, 2), noise_sd = 1.0) {\n  x &lt;- runif(n, -3, 3)\n  y &lt;- beta[1] + beta[2] * x^2 + rnorm(n, sd = noise_sd)\n  return(list(x = x, y = y))\n}\n\n# Storage for results\nridge_nonlinear_results &lt;- data.frame()\n\n# Simulation\nfor (n in sample_sizes) {\n  beta_estimates &lt;- matrix(NA, nrow = num_simulations, ncol = 2)\n  in_sample_mse &lt;- numeric(num_simulations)\n  out_sample_mse &lt;- numeric(num_simulations)\n\n  for (sim in 1:num_simulations) {\n    # Generate training data\n    train &lt;- generate_nonlinear_dgp(n)\n    x &lt;- train$x\n    y &lt;- train$y\n    X_quad &lt;- cbind(1, x^2)\n\n    # Fit Ridge regression\n    ridge_model &lt;- glmnet(x = X_quad, y = y, alpha = 0, lambda = lambda_ridge,\n                          intercept = FALSE, standardize = FALSE)\n    beta_hat &lt;- as.vector(coef(ridge_model, s = lambda_ridge))[-1]\n    beta_estimates[sim, ] &lt;- beta_hat\n\n    # In-sample MSE\n    y_pred &lt;- X_quad %*% beta_hat\n    in_sample_mse[sim] &lt;- mean((y - y_pred)^2)\n\n    # Test set\n    test &lt;- generate_nonlinear_dgp(n)\n    x_test &lt;- test$x\n    y_test &lt;- test$y\n    X_test_quad &lt;- cbind(1, x_test^2)\n    y_test_pred &lt;- X_test_quad %*% beta_hat\n    out_sample_mse[sim] &lt;- mean((y_test - y_test_pred)^2)\n  }\n\n  # Bias and variance for β1 (quadratic term)\n  beta1_hat &lt;- beta_estimates[, 2]\n  bias_beta1 &lt;- mean(beta1_hat) - beta[2]\n  variance_beta1 &lt;- var(beta1_hat)\n\n  # Store results\n  ridge_nonlinear_results &lt;- rbind(ridge_nonlinear_results, data.frame(\n    sample_size = n,\n    bias_beta1 = bias_beta1,\n    variance_beta1 = variance_beta1,\n    in_sample_mse = mean(in_sample_mse),\n    out_sample_mse = mean(out_sample_mse)\n  ))\n}\n\n# Bias\np1 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = bias_beta1)) +\n  geom_line(color = \"darkred\") +\n  geom_point(color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Bias of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Bias\")\n\n# Variance\np2 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = variance_beta1)) +\n  geom_line(color = \"darkblue\") +\n  geom_point(color = \"darkblue\") +\n  labs(title = \"Variance of β1 vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"Variance\")\n\n# In-sample MSE\np3 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = in_sample_mse)) +\n  geom_line(color = \"darkgreen\") +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"In-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Out-of-sample MSE\np4 &lt;- ggplot(ridge_nonlinear_results, aes(x = sample_size, y = out_sample_mse)) +\n  geom_line(color = \"darkmagenta\") +\n  geom_point(color = \"darkmagenta\") +\n  labs(title = \"Out-of-Sample MSE vs. Sample Size (Ridge - Non-Linear)\",\n       x = \"Sample Size (n)\", y = \"MSE\")\n\n# Display 2x2 grid\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\n Bias and Variance of non-Linear DGP using Monte Carlo simulations\n\nThe bias is now nearly constant as seen in the top-left, while the in-and-out of sample errors seem to still follow the trends we saw with the last two."
  }
]