{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import and clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file's only purpose is to read in and clean the data to our standard train/test data for 9890 Prediction Competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data, import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import src.workfile_functions as wf\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns',50)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "PATH = '../data/pc/'\n",
    "\n",
    "URL = \"https://michael-weylandt.com/STA9890/competition_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in from prof's website\n",
    "BUILDING_DETAILS_2015 = wf.read_gh(URL, \"building_details_2015.csv.gz\")\n",
    "BUILDING_DETAILS_2016 = wf.read_gh(URL, \"building_details_2016.csv.gz\")\n",
    "BUILDING_DETAILS_2017 = wf.read_gh(URL, \"building_details_2017.csv.gz\")\n",
    "BUILDING_DETAILS_2018 = wf.read_gh(URL, \"building_details_2018.csv.gz\")\n",
    "BUILDING_DETAILS_2019 = wf.read_gh(URL, \"building_details_2019.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SAMPLES = wf.read_gh(URL,\"assessment_history_train.csv.gz\")\n",
    "TEST_POINTS = wf.read_gh(URL,\"assessment_history_test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15 = wf.clean_building_dfs(BUILDING_DETAILS_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16 = wf.clean_building_dfs(BUILDING_DETAILS_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df17 = wf.clean_building_dfs(BUILDING_DETAILS_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18 = wf.clean_building_dfs(BUILDING_DETAILS_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df19 = wf.clean_building_dfs(BUILDING_DETAILS_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Account IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df15[['acct']].copy()\n",
    "df_long['year'] = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will add new accounts to df_long\n",
    "# based on their appearances in dfs 16 through 19\n",
    "def append_new_accts(df_base, df_new, year):\n",
    "    existing_accts = set(df_base['acct'])\n",
    "    new_entries = df_new[~df_new['acct'].isin(existing_accts)][['acct']].copy()\n",
    "    new_entries['year'] = year\n",
    "    return pd.concat([df_base,new_entries],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, yr in zip([df16, df17, df18, df19], [2016, 2017, 2018, 2019]):\n",
    "    df_long = append_new_accts(df_long, df, yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are columns that should remain unchanged through a home's assesment history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = [\n",
    "    'year_built',\n",
    "    'foundation_Basement',\n",
    "    'foundation_Crawl Space',\n",
    "    'foundation_Slab',\n",
    "    'foundation_Mixed'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df_map = {\n",
    "    2015: df15,\n",
    "    2016: df16,\n",
    "    2017: df17,\n",
    "    2018: df18,\n",
    "    2019: df19\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in static_cols:\n",
    "    df_long[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, df_source in year_df_map.items():\n",
    "    # Get only accounts from df_long for that year\n",
    "    acct_subset = df_long[df_long['year'] == year][['acct']]\n",
    "\n",
    "    # Pull static columns from the matching year's DataFrame\n",
    "    df_extract = df_source[['acct'] + static_cols]\n",
    "\n",
    "    # Merge on acct (only updates matching rows)\n",
    "    df_long = df_long.merge(df_extract, on='acct', how='left', suffixes=('', '_tmp'))\n",
    "\n",
    "    # Only assign static values where year matches (to avoid overwriting)\n",
    "    for col in static_cols:\n",
    "        df_long.loc[df_long['year'] == year, col] = df_long.loc[df_long['year'] == year, f'{col}_tmp']\n",
    "        df_long.drop(columns=[f'{col}_tmp'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = df15.columns.tolist()\n",
    "exclude_cols = ['acct', 'year'] + static_cols  # static_cols from before\n",
    "first_cols = [col for col in all_columns if col not in exclude_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, df_source in year_df_map.items():\n",
    "    acct_subset = df_long[df_long['year'] == year][['acct']]\n",
    "\n",
    "    # Pull and rename first_cols → tmp cols\n",
    "    df_extract = df_source[['acct'] + first_cols].copy()\n",
    "    df_extract.rename(columns={col: f\"{col}_tmp\" for col in first_cols}, inplace=True)\n",
    "\n",
    "    # Merge by acct\n",
    "    df_long = df_long.merge(df_extract, on='acct', how='left')\n",
    "\n",
    "    # Assign year-specific values\n",
    "    for col in first_cols:\n",
    "        tmp_col = f'{col}_tmp'\n",
    "        new_col = f'first_{col}'\n",
    "\n",
    "        if new_col not in df_long:\n",
    "            df_long[new_col] = np.nan\n",
    "\n",
    "        df_long.loc[df_long['year'] == year, new_col] = df_long.loc[df_long['year'] == year, tmp_col]\n",
    "        df_long.drop(columns=[tmp_col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a set to track seen accounts\n",
    "seen_accts = set()\n",
    "\n",
    "# Start with a copy of df_long to add 'end_' columns to\n",
    "df_long_end = df_long.copy()\n",
    "\n",
    "# Create placeholders for all end_ columns\n",
    "for col in first_cols:\n",
    "    df_long_end[f'end_{col}'] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for \"Last Year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop in reverse order: 2019 → 2015\n",
    "for year, df_source in reversed(list(year_df_map.items())):\n",
    "    # Find accounts in this year that haven't been assigned yet\n",
    "    df_year_accts = df_source[['acct']].copy()\n",
    "    new_accts = df_year_accts[~df_year_accts['acct'].isin(seen_accts)]\n",
    "\n",
    "    # Keep track of which accounts we've already assigned from later years\n",
    "    seen_accts.update(new_accts['acct'])\n",
    "\n",
    "    # Pull and rename cols → tmp\n",
    "    df_extract = df_source[['acct'] + first_cols].copy()\n",
    "    df_extract.rename(columns={col: f'{col}_tmp' for col in first_cols}, inplace=True)\n",
    "\n",
    "    # Merge with current df_long_end\n",
    "    df_long_end = df_long_end.merge(df_extract, on='acct', how='left')\n",
    "\n",
    "    # Assign values for those just-seen accounts\n",
    "    for col in first_cols:\n",
    "        tmp_col = f'{col}_tmp'\n",
    "        end_col = f'end_{col}'\n",
    "\n",
    "        mask = df_long_end['acct'].isin(new_accts['acct'])\n",
    "        df_long_end.loc[mask, end_col] = df_long_end.loc[mask, tmp_col]\n",
    "\n",
    "        df_long_end.drop(columns=[tmp_col], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in first_cols:\n",
    "    start_col = f'first_{col}'\n",
    "    end_col = f'end_{col}'\n",
    "    delta_col = f'delta_{col}'\n",
    "\n",
    "    df_long_end[delta_col] = df_long_end[end_col] - df_long_end[start_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with Train/Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wf.read_gh(URL,\"assessment_history_train.csv.gz\")\n",
    "test = wf.read_gh(URL,\"assessment_history_test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(df_long, on = 'acct', how = 'left')\n",
    "test = test.merge(df_long, on = 'acct', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = train.select_dtypes(include='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols.remove('acct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots = ['protested_2015','protested_2016','protested_2017','protested_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[prots] = train[prots].fillna(False).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[prots] = test[prots].fillna(False).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in prots:\n",
    "    object_cols.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in object_cols:\n",
    "    freq = train[col].value_counts()\n",
    "    train[f'{col}_freq'] = train[col].map(freq)\n",
    "    test[f'{col}_freq'] = test[col].map(freq)  # or use train's freq to avoid leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(object_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(object_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NULL Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "for col in missing_cols:\n",
    "    df_not_missing = train[train[col].notnull()]\n",
    "    df_missing = train[train[col].isnull()]\n",
    "\n",
    "    features = [f for f in train.columns if f != col]\n",
    "    X_missing = df_missing[features].select_dtypes(exclude='object')\n",
    "\n",
    "    # Sample training data\n",
    "    df_sample = df_not_missing.sample(n=100000, random_state=42)\n",
    "    X_train = df_sample[features].select_dtypes(exclude='object')\n",
    "    y_train = df_sample[col]\n",
    "\n",
    "    model = HistGradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_missing)\n",
    "    train.loc[train[col].isnull(), col] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['building_area_2015', 'land_area_2015', 'building_area_2016',\n",
      "       'land_area_2016', 'building_area_2017', 'land_area_2017',\n",
      "       'building_area_2018', 'land_area_2018', 'building_value_2015',\n",
      "       'land_value_2015',\n",
      "       ...\n",
      "       'delta_brick_masonry', 'delta_concrete_block', 'delta_vinyl',\n",
      "       'delta_stucco', 'delta_stone', 'delta_other', 'region_freq',\n",
      "       'zone_freq', 'subneighborhood_freq', 'neighborhood_freq'],\n",
      "      dtype='object', length=117)\n"
     ]
    }
   ],
   "source": [
    "missing_cols = test.columns[test.isnull().any()]\n",
    "print(missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in missing_cols:\n",
    "    df_not_missing = test[test[col].notnull()]\n",
    "    df_missing = test[test[col].isnull()]\n",
    "\n",
    "    features = [f for f in test.columns if f != col]\n",
    "    X_missing = df_missing[features].select_dtypes(exclude='object')\n",
    "\n",
    "    # Sample training data\n",
    "    df_sample = df_not_missing.sample(n=100000, random_state=42)\n",
    "    X_train = df_sample[features].select_dtypes(exclude='object')\n",
    "    y_train = df_sample[col]\n",
    "\n",
    "    model = HistGradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_missing)\n",
    "    test.loc[test[col].isnull(), col] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/pc/train.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../data/pc/test.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc_Python=3.10.0",
   "language": "python",
   "name": "pc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
