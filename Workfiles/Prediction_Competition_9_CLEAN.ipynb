{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries Needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',50)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "PATH = '../data/pc/'\n",
    "\n",
    "URL = \"https://michael-weylandt.com/STA9890/competition_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv files from site\n",
    "def read_gh(URL, f):\n",
    "    return pd.read_csv(URL + f)\n",
    "\n",
    "def group_rare_categories(df, column, threshold=100, new_label='Other'):\n",
    "    \"\"\"\n",
    "    Replace categories in a column that appear fewer than `threshold` times with `new_label`.\n",
    "    \"\"\"\n",
    "    value_counts = df[column].value_counts()\n",
    "    rare_values = value_counts[value_counts < threshold].index\n",
    "    df[column] = df[column].apply(lambda x: new_label if x in rare_values else x)\n",
    "    return df\n",
    "\n",
    "# appends new accounts to running list of unique accounts\n",
    "def append_new_accts(df_base, df_new, year):\n",
    "    existing_accts = set(df_base['acct'])\n",
    "    new_entries = df_new[~df_new['acct'].isin(existing_accts)][['acct']].copy()\n",
    "    new_entries['year'] = year\n",
    "    return pd.concat([df_base,new_entries],ignore_index=True)\n",
    "\n",
    "# Cleans the Building DATAFRAME files\n",
    "def clean_building_dfs(df):\n",
    "    tfcols = ['elevator','has_cooling','has_heat']\n",
    "    for col in tfcols:\n",
    "        df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "    quality_map = {\n",
    "    'F':0,\n",
    "    'E':1,\n",
    "    'D':2,\n",
    "    'C':3,\n",
    "    'B':4,\n",
    "    'A':5,\n",
    "    'X':6\n",
    "    }\n",
    "\n",
    "    df['quality'] = df['quality'].map(quality_map)\n",
    "\n",
    "    quality_description_map = {\n",
    "        'Poor':0,\n",
    "        'Very Low':1,\n",
    "        'Low':2,\n",
    "        'Average':3,\n",
    "        'Good':4,\n",
    "        'Excellent':5,\n",
    "        'Superior':6\n",
    "    }\n",
    "\n",
    "    df['quality_description'] = df['quality_description'].map(quality_description_map)\n",
    "\n",
    "    bc_map = {\n",
    "        'Unsound':0,\n",
    "        'Very Poor':1,\n",
    "        'Poor':2,\n",
    "        'Fair':3,\n",
    "        'Average':4,\n",
    "        'Good':5,\n",
    "        'Very Good':6,\n",
    "        'Excellent':7\n",
    "    }\n",
    "\n",
    "    df['building_condition'] = df['building_condition'].map(bc_map)\n",
    "\n",
    "    grade_map = {\n",
    "        'E-':1.3,\n",
    "        'E':1.6,\n",
    "        'E+':1.9,\n",
    "        'D-':2.3,\n",
    "        'D':2.6,\n",
    "        'D+':2.9,\n",
    "        'C-':3.3,\n",
    "        'C':3.6,\n",
    "        'C+':3.9,\n",
    "        'B-':4.3,\n",
    "        'B':4.6,\n",
    "        'B+':4.9,\n",
    "        'A-':5.3,\n",
    "        'A':5.6,\n",
    "        'A+':5.9,\n",
    "        'X-':6.3,\n",
    "        'X':6.6,\n",
    "        'X+':6.9\n",
    "    }\n",
    "\n",
    "    df['grade'] = df['grade'].map(grade_map)\n",
    "\n",
    "    pc_map = {\n",
    "        'Unsound':0,\n",
    "        'Very Poor':1,\n",
    "        'Poor':2,\n",
    "        'Fair':3,\n",
    "        'Average':4,\n",
    "        'Good':5,\n",
    "        'Very Good':6,\n",
    "        'Excellent':7\n",
    "    }\n",
    "\n",
    "    df['physical_condition'] = df['physical_condition'].map(pc_map)\n",
    "\n",
    "    df.loc[df['foundation_type'] == 'Basement and Basement', 'foundation_type'] = 'Basement'\n",
    "\n",
    "    df.loc[(df['foundation_type']=='Basement and Slab')|(df['foundation_type']=='Crawl Space and Slab')|(df['foundation_type']=='Basement and Crawl Space')|\n",
    "           (df['foundation_type']=='Basement and Pier and Beam')|(df['foundation_type']=='Pier and Beam')|\n",
    "           (df['foundation_type']=='Pier and Beam and Pier and Beam')|(df['foundation_type']=='Pier and Beam and Slab'),'foundation_type'] = 'Mixed'\n",
    "\n",
    "    dummies = pd.get_dummies(df['foundation_type'], prefix='foundation').astype('Int64')\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    df = group_rare_categories(df, 'exterior_walls', threshold=1000, new_label='Other')\n",
    "\n",
    "    # Keywords to extract\n",
    "    keywords = [\n",
    "        'Brick Veneer',\n",
    "        'Brick Masonry',\n",
    "        'Concrete Block',\n",
    "        'Vinyl',\n",
    "        'Stucco',\n",
    "        'Stone',\n",
    "        'Other'\n",
    "    ]\n",
    "\n",
    "    # Create 1/0 dummy columns based on substring presence\n",
    "    for key in keywords:\n",
    "        col_name = key.replace(' ', '_').lower()  # e.g., 'Brick Veneer' → 'brick_veneer'\n",
    "        df[col_name] = df['exterior_walls'].fillna('').str.contains(key).astype('int8')\n",
    "\n",
    "    df = df.drop(['exterior_walls','foundation_type'],axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to count occurrances of a specific string value\n",
    "def count_values(df,value:str):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns printout of whatever character you're looking for across the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    total_rows = len(df)\n",
    "    val_counts = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        count = (df[col] == value).sum()\n",
    "        if count > 0:\n",
    "            percent = (count / total_rows) * 100\n",
    "            val_counts.append((col, count, percent))\n",
    "\n",
    "    # Sort by count descending\n",
    "    val_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{'Column':<30} {'? Count':>15} {'% of Rows':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for col, count, percent in val_counts:\n",
    "        print(f\"{col:<30} {count:>15,} {percent:>11.2f}%\")\n",
    "\n",
    "# function to count NULLs in the columns of a df\n",
    "def count_nulls(df, top_n=10):\n",
    "    total_rows = len(df)\n",
    "    null_counts = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        count = df[col].isna().sum()\n",
    "        if count > 0:\n",
    "            percent = (count / total_rows) * 100\n",
    "            null_counts.append((col, count, percent))\n",
    "\n",
    "    # Sort by null count descending\n",
    "    null_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print total number of columns with NULLs\n",
    "    print(f\"Total number of columns with NULLs: {len(null_counts)}\\n\")\n",
    "\n",
    "    # If there are NULLs, display formatted table\n",
    "    if null_counts:\n",
    "        print(f\"{'Column':<30} {'NULL Count':>15} {'% of Rows':>12}\")\n",
    "        print(\"-\" * 60)\n",
    "        for col, count, percent in null_counts[:top_n]:\n",
    "            print(f\"{col:<30} {count:>15,} {percent:>11.2f}%\")\n",
    "    else:\n",
    "        print(\"No NULLs found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores():\n",
    "\n",
    "    data = {\n",
    "      'Model': [\n",
    "          'OLS', 'LASSO', 'Ridge', 'RidgeCV',\n",
    "          'XGBR','XGBR_GS','LGBM','Ensemble',\n",
    "          'Stack_1','Stack_2','Stack_3'\n",
    "          ],\n",
    "      'Train RMSE': [\n",
    "          42103.47, 48757.61, 42101.38, 42101.38,\n",
    "          24121.34, 40700.99, 42000.00, 41068.07,\n",
    "          43313.99, 37393.21, 33441.12\n",
    "          ],\n",
    "      'Test RMSE (Public)': [\n",
    "          48478.35, 46676.85, 48857.30, 47080.00,\n",
    "          42143.88, 43196.91, 170263.72, 44709.20,\n",
    "          43589.06, 42896.77, 45241.33\n",
    "          ],\n",
    "      'Test RMSE (Private)': [\n",
    "          46903.05, 45345.82, 47746.13, 45075.56,\n",
    "          43759.64, 44054.83, 174621.3, 44458.82,\n",
    "          44946.14, 44380.56, 44128.42\n",
    "          ]\n",
    "      }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df_melted = df.melt(id_vars='Model', value_vars=['Train RMSE', 'Test RMSE (Public)', 'Test RMSE (Private)'],\n",
    "                    var_name='Metric', value_name='RMSE')\n",
    "\n",
    "    sns.set(style=\"whitegrid\", rc={\"axes.facecolor\": \"#ebebeb\"})  # slightly grayer\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "  # sns.set(style=\"whitegrid\", rc={\"axes.facecolor\": \"#d3d3d3\"})  # slightly darker gray\n",
    "    sns.set(style=\"whitegrid\", rc={\n",
    "      \"axes.facecolor\": \"#d3d3d3\",    # inside the plot\n",
    "      \"figure.facecolor\": \"#d3d3d3\",  # everything around it\n",
    "      \"grid.color\": \"black\",\n",
    "      \"grid.linewidth\": 0.5\n",
    "  })\n",
    "\n",
    "    bar = sns.barplot(data=df_melted, x='Model', y='RMSE', hue='Metric', palette='mako')\n",
    "\n",
    "  # Add value labels\n",
    "  # for p in bar.patches:\n",
    "  #     height = p.get_height()\n",
    "  #     if not pd.isna(height):\n",
    "  #         bar.annotate(f'{height:,.0f}',\n",
    "  #                      (p.get_x() + p.get_width() / 2, height),\n",
    "  #                      ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "  # Add horizontal line for best (lowest) Test RMSE\n",
    "    min_test_rmse_pub = df['Test RMSE (Public)'].min()\n",
    "    plt.axhline(min_test_rmse_pub, linestyle='--', color='darkgreen', linewidth=1.5, label='Best Test RMSE (Public) = 42,144')\n",
    "\n",
    "  # same but for private leaderboard\n",
    "    min_test_rmse_priv = df['Test RMSE (Private)'].min()\n",
    "    plt.axhline(min_test_rmse_priv, linestyle='--', color='darkblue', linewidth=1.5, label='Best Test RMSE (Private) = 43,760')\n",
    "\n",
    "    plt.grid(True, color='black', linewidth=0.5)\n",
    "\n",
    "    # Bold titles and labels\n",
    "    plt.legend()\n",
    "    plt.title(\"Train vs Test RMSE by Model\", fontweight='bold', fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontweight='bold')\n",
    "    plt.xlabel(\"Model\", fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in from prof's website\n",
    "BUILDING_DETAILS_2015 = read_gh(URL, \"building_details_2015.csv.gz\")\n",
    "BUILDING_DETAILS_2016 = read_gh(URL, \"building_details_2016.csv.gz\")\n",
    "BUILDING_DETAILS_2017 = read_gh(URL, \"building_details_2017.csv.gz\")\n",
    "BUILDING_DETAILS_2018 = read_gh(URL, \"building_details_2018.csv.gz\")\n",
    "BUILDING_DETAILS_2019 = read_gh(URL, \"building_details_2019.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15 = clean_building_dfs(BUILDING_DETAILS_2015)\n",
    "df16 = clean_building_dfs(BUILDING_DETAILS_2016)\n",
    "df17 = clean_building_dfs(BUILDING_DETAILS_2017)\n",
    "df18 = clean_building_dfs(BUILDING_DETAILS_2018)\n",
    "df19 = clean_building_dfs(BUILDING_DETAILS_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df15[['acct']].copy()\n",
    "df_long['year'] = 2015\n",
    "\n",
    "for df, yr in zip([df16, df17, df18, df19], [2016, 2017, 2018, 2019]):\n",
    "    df_long = append_new_accts(df_long, df, yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = [\n",
    "    'year_built',\n",
    "    'foundation_Basement',\n",
    "    'foundation_Crawl Space',\n",
    "    'foundation_Slab',\n",
    "    'foundation_Mixed'\n",
    "]\n",
    "\n",
    "year_df_map = {\n",
    "    2015: df15,\n",
    "    2016: df16,\n",
    "    2017: df17,\n",
    "    2018: df18,\n",
    "    2019: df19\n",
    "}\n",
    "\n",
    "for col in static_cols:\n",
    "    df_long[col] = np.nan\n",
    "    \n",
    "for year, df_source in year_df_map.items():\n",
    "    # Get only accounts from df_long for that year\n",
    "    acct_subset = df_long[df_long['year'] == year][['acct']]\n",
    "\n",
    "    # Pull static columns from the matching year's DataFrame\n",
    "    df_extract = df_source[['acct'] + static_cols]\n",
    "\n",
    "    # Merge on acct (only updates matching rows)\n",
    "    df_long = df_long.merge(df_extract, on='acct', how='left', suffixes=('', '_tmp'))\n",
    "\n",
    "    # Only assign static values where year matches (to avoid overwriting)\n",
    "    for col in static_cols:\n",
    "        df_long.loc[df_long['year'] == year, col] = df_long.loc[df_long['year'] == year, f'{col}_tmp']\n",
    "        df_long.drop(columns=[f'{col}_tmp'], inplace=True)\n",
    "\n",
    "all_columns = df15.columns.tolist()\n",
    "exclude_cols = ['acct', 'year'] + static_cols  # static_cols from before\n",
    "first_cols = [col for col in all_columns if col not in exclude_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, df_source in year_df_map.items():\n",
    "    acct_subset = df_long[df_long['year'] == year][['acct']]\n",
    "\n",
    "    # Rename first_cols\n",
    "    df_extract = df_source[['acct'] + first_cols].copy()\n",
    "    df_extract.rename(columns={col: f\"{col}_tmp\" for col in first_cols}, inplace=True)\n",
    "\n",
    "    # Merge by acct\n",
    "    df_long = df_long.merge(df_extract, on='acct', how='left')\n",
    "\n",
    "    # Assign year-specific values\n",
    "    for col in first_cols:\n",
    "        tmp_col = f'{col}_tmp'\n",
    "        new_col = f'first_{col}'\n",
    "\n",
    "        if new_col not in df_long:\n",
    "            df_long[new_col] = np.nan\n",
    "\n",
    "        df_long.loc[df_long['year'] == year, new_col] = df_long.loc[df_long['year'] == year, tmp_col]\n",
    "        df_long.drop(columns=[tmp_col], inplace=True)\n",
    "        \n",
    "# Track seen accounts\n",
    "seen_accts = set()\n",
    "\n",
    "df_long_end = df_long.copy()\n",
    "\n",
    "# Create placeholders for all end_ columns\n",
    "for col in first_cols:\n",
    "    df_long_end[f'end_{col}'] = np.nan\n",
    "    \n",
    "# Loop in reverse order\n",
    "for year, df_source in reversed(list(year_df_map.items())):\n",
    "    # Find accounts in this year that haven't been assigned yet\n",
    "    df_year_accts = df_source[['acct']].copy()\n",
    "    new_accts = df_year_accts[~df_year_accts['acct'].isin(seen_accts)]\n",
    "\n",
    "    # Keep track of which accounts we've already assigned from later years\n",
    "    seen_accts.update(new_accts['acct'])\n",
    "\n",
    "    # Pull and rename cols → tmp\n",
    "    df_extract = df_source[['acct'] + first_cols].copy()\n",
    "    df_extract.rename(columns={col: f'{col}_tmp' for col in first_cols}, inplace=True)\n",
    "\n",
    "    # Merge with current df_long_end\n",
    "    df_long_end = df_long_end.merge(df_extract, on='acct', how='left')\n",
    "\n",
    "    # Assign values for those just-seen accounts\n",
    "    for col in first_cols:\n",
    "        tmp_col = f'{col}_tmp'\n",
    "        end_col = f'end_{col}'\n",
    "\n",
    "        mask = df_long_end['acct'].isin(new_accts['acct'])\n",
    "        df_long_end.loc[mask, end_col] = df_long_end.loc[mask, tmp_col]\n",
    "\n",
    "        df_long_end.drop(columns=[tmp_col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046929"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in first_cols:\n",
    "    start_col = f'first_{col}'\n",
    "    end_col = f'end_{col}'\n",
    "    delta_col = f'delta_{col}'\n",
    "\n",
    "    df_long_end[delta_col] = df_long_end[end_col] - df_long_end[start_col]\n",
    "\n",
    "df = df_long_end.copy()\n",
    "\n",
    "train = read_gh(URL,\"assessment_history_train.csv.gz\")\n",
    "test = read_gh(URL,\"assessment_history_test.csv.gz\")\n",
    "\n",
    "train = train.merge(df, on = 'acct', how = 'left')\n",
    "test = test.merge(df, on = 'acct', how = 'left')\n",
    "\n",
    "df_long_end['acct'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['region',\n",
       " 'protested_2015',\n",
       " 'protested_2016',\n",
       " 'protested_2017',\n",
       " 'protested_2018',\n",
       " 'zone',\n",
       " 'subneighborhood',\n",
       " 'neighborhood']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_cols = train.select_dtypes(include='object').columns.tolist()\n",
    "object_cols.remove('acct')\n",
    "\n",
    "object_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prots = ['protested_2015','protested_2016','protested_2017','protested_2018']\n",
    "train[prots] = train[prots].fillna(False).astype('int8')\n",
    "\n",
    "for c in prots:\n",
    "    object_cols.remove(c)\n",
    "    \n",
    "for col in object_cols:\n",
    "    freq = train[col].value_counts()\n",
    "    train[f'{col}_freq'] = train[col].map(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = test.select_dtypes(include='object').columns.tolist()\n",
    "object_cols.remove('acct')\n",
    "\n",
    "test[prots] = test[prots].fillna(False).astype('int8')\n",
    "\n",
    "for col in object_cols:\n",
    "    freq = test[col].value_counts()\n",
    "    test[f'{col}_freq'] = test[col].map(freq)\n",
    "    \n",
    "train.drop(object_cols,axis=1,inplace=True)\n",
    "test.drop(object_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_floor_area_primary</th>\n",
       "      <th>end_floor_area_primary</th>\n",
       "      <th>delta_floor_area_primary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536503</th>\n",
       "      <td>1315.0</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385834</th>\n",
       "      <td>1764.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524083</th>\n",
       "      <td>1050.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248515</th>\n",
       "      <td>1960.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        first_floor_area_primary  end_floor_area_primary  \\\n",
       "536503                    1315.0                  1315.0   \n",
       "385834                    1764.0                  1800.0   \n",
       "524083                    1050.0                  1050.0   \n",
       "248515                    1960.0                  1960.0   \n",
       "\n",
       "        delta_floor_area_primary  \n",
       "536503                       0.0  \n",
       "385834                      36.0  \n",
       "524083                       0.0  \n",
       "248515                       0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['first_floor_area_primary','end_floor_area_primary','delta_floor_area_primary']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.897983\n",
       "0.0    0.102017\n",
       "Name: foundation_Slab, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['foundation_Slab'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0      2099\n",
       "2.0     46851\n",
       "3.0    382835\n",
       "4.0    150137\n",
       "5.0     36518\n",
       "6.0      5709\n",
       "NaN      4138\n",
       "Name: end_quality, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['end_quality'].value_counts(dropna=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of columns with NULLs: 114\n",
      "\n",
      "Column                              NULL Count    % of Rows\n",
      "------------------------------------------------------------\n",
      "delta_quality                          594,830       94.67%\n",
      "delta_quality_description              594,830       94.67%\n",
      "delta_building_condition               594,807       94.67%\n",
      "delta_grade                            594,807       94.67%\n",
      "delta_physical_condition               594,807       94.67%\n",
      "first_quality                          594,795       94.67%\n",
      "first_quality_description              594,795       94.67%\n",
      "first_building_condition               594,787       94.67%\n",
      "first_grade                            594,787       94.67%\n",
      "first_physical_condition               594,787       94.67%\n"
     ]
    }
   ],
   "source": [
    "count_nulls(train,top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "missing_cols = train.columns[train.isnull().any()]\n",
    "print(len(missing_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-1783db88c109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Sample training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_not_missing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[0;32m   4991\u001b[0m             )\n\u001b[0;32m   4992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4993\u001b[1;33m         \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4994\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "for col in missing_cols:\n",
    "    df_not_missing = train[train[col].notnull()]\n",
    "    df_missing = train[train[col].isnull()]\n",
    "\n",
    "    features = [f for f in train.columns if f != col]\n",
    "    X_missing = df_missing[features].select_dtypes(exclude='object')\n",
    "\n",
    "    # Sample training data\n",
    "    df_sample = df_not_missing.sample(n=100000, random_state=42)\n",
    "    X_train = df_sample[features].select_dtypes(exclude='object')\n",
    "    y_train = df_sample[col]\n",
    "\n",
    "    model = HistGradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_missing)\n",
    "    train.loc[train[col].isnull(), col] = preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-cc88f688b3a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Sample training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_not_missing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[0;32m   4991\u001b[0m             )\n\u001b[0;32m   4992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4993\u001b[1;33m         \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4994\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "for col in missing_cols:\n",
    "    df_not_missing = test[test[col].notnull()]\n",
    "    df_missing = test[test[col].isnull()]\n",
    "\n",
    "    features = [f for f in test.columns if f != col]\n",
    "    X_missing = df_missing[features].select_dtypes(exclude='object')\n",
    "\n",
    "    # Sample training data\n",
    "    df_sample = df_not_missing.sample(n=100000, random_state=42)\n",
    "    X_train = df_sample[features].select_dtypes(exclude='object')\n",
    "    y_train = df_sample[col]\n",
    "\n",
    "    model = HistGradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_missing)\n",
    "    test.loc[test[col].isnull(), col] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing some duplicate values\n",
    "train = train.drop_duplicates(subset='acct')\n",
    "\n",
    "X = train.drop(columns=['acct','TARGET'])\n",
    "y = train['TARGET']\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X, y)\n",
    "\n",
    "pred_xgb = xgb.predict(X)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y, pred_xgb))\n",
    "\n",
    "print(f\"XGBoost RMSE: {rmse_xgb:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc_Python=3.10.0",
   "language": "python",
   "name": "pc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
